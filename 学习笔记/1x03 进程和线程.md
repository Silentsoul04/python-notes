# 进程和线程

对操作系统而言，可将一个任务视为一个**进程** Process。比如打开某个程序，便会启动一个相应的进程。同一进程内的“子任务”，则可视为**线程** Thread。
线程是最小的执行单元，而进程由至少一个线程组成。

多任务的实现方式：多进程模式；多线程模式；多进程+多线程模式。

同时执行的各个任务间通常都会存在某种联系，这便需要进程间的相互协作。
比如，有时任务A 必须等待任务B完成后才能继续执行。
另外，多进程和多线程的程序涉及到同步、数据共享等问题，编写起来更复杂。
所以，多进程+多线程模式的复杂度远高于单进程单线程的模式。

进程和线程的调度，完全由操作系统决定，程序并不能决定其自身什么时候被执行，也不能决定被执行多长时间。

Python 既支多进程模式和多线程模式

## 1. 多进程 multiprocessing

Unix/Linux 系统下，可使用 `fork()` 实现多进程。
`multiprocessing` 模块可跨平台实现多进程。

### os.fork() for Unix 

os 模块用于封装常见的系统调用。

-   os.fork() 

    Fork a child process. Return `0` in the child and the child’s process id in the parent. If an error occurs [`OSError`](exceptions.html#OSError) is raised.Note that some platforms including FreeBSD <= 6.3 and Cygwin **have known issues** when using fork() from a thread.

    Warning : See [`ssl`](ssl.html#module-ssl) for applications that use the SSL module with fork().

    **Availability**: Unix. 仅在 Unix 下可用。    


当调用 `os.fork()` 时，操作系统会自动将当前进程作为父进程 fork 出一个子进程。
因此，每调用一次 `os.fork()` 会在子进程和父进程内分别返回一次。

-   在子进程中永远返回 `0` 。在子进程中调用 `os.getppid()` ，可拿到父进程的 ID。
-   在父进程中返回子进程的 ID。( 理由：父进程可以 fork 出多个子进程，这便于父进程记录子进程的 ID )

```
#本示例只适用于Unix
import os

print('Process (%s) start...' % os.getpid())
# Only works on Unix/Linux/Mac:
pid = os.fork()
if pid == 0:
    print('I am child process (%s) and my parent is %s.' % (os.getpid(), os.getppid()))
else:
    print('I (%s) just created a child process (%s).' % (os.getpid(), pid))
```

运行结果如下：

```
Process (876) start...
I (876) just created a child process (877).
I am child process (877) and my parent is 876.
```

常见的 Apache 服务器就是由父进程监听端口，每当有新的 http 请求时，就 fork 出子进程来处理新请求。

#### os.getpid() 

Return the current process id.

#### os.getppid() 

Return the parent’s process id. When the parent process has exited, on Unix the id returned is the one of the init process (1), on Windows it is still the same id, which may be already reused by another process.
Availability: Unix, Windows.
Changed in version 3.2: Added support for Windows.


### multiprocessing 模块

`multiprocessing` 模块可跨平台实现多进程。[¶](https://docs.python.org/3/library/multiprocessing.html#module-multiprocessing)

#### Process 类

Process 类的实例表示一个进程对象。

-   *class* multiprocessing.**Process**(*group=None*, *target=None*, *name=None*, *args=()*, *kwargs={}*, \*, *daemon=None*)  

    Process objects represent activity that is run in a separate process. 
    process 对象表示在独立进程中运行的活动。
    The [`Process`](#multiprocessing.Process) class has equivalents of all the methods of [`threading.Thread`](threading.html#threading.Thread).

    The constructor构造器 should always be called with keyword arguments. 

    -   *group* should always be `None`; it exists solely for compatibility with [`threading.Thread`](threading.html#threading.Thread). 该参数的存在仅仅是为了和 `threading.Thread` 兼容。
    -   *target* is the callable object to be invoked by the [`run()`](#multiprocessing.Process.run) method. It defaults to `None`, meaning nothing is called. 
    -   *name* is the process name (see [`name`](#multiprocessing.Process.name) for more details). 
    -   *args* is the argument tuple for the target invocation. 
    -   *kwargs* is a dictionary of keyword arguments for the target invocation. 
    -   If provided, the keyword-only *daemon* 守护进程 argument sets the process [`daemon`](#multiprocessing.Process.daemon) flag to `True` or `False`. If `None` (the default), this flag will be inherited from the creating process.

    By default, no arguments are passed to *target*.

    If a subclass overrides the constructor, it must make sure it invokes the base class constructor (`Process.__init__()`) before doing anything else to the process.

    Changed in version 3.3: Added the *daemon* argument.

##### run()[¶](https://docs.python.org/3/library/multiprocessing.html#multiprocessing.Process.run)

Method representing the process’s activity.

You may override this method in a subclass. The standard [`run()`](https://docs.python.org/3/library/multiprocessing.html#multiprocessing.Process.run) method invokes the callable object passed to the object’s constructor as the *target* argument, if any, with sequential and keyword arguments taken from the *args* and *kwargs* arguments, respectively.

##### start()

Start the process’s activity.
This must be called at most once per process object. 
每个进程对象最多只能调用一次。

It arranges for the object’s [`run()`](https://docs.python.org/3/library/multiprocessing.html#multiprocessing.Process.run) method to be invoked in a separate process.
该方法将进程实例的 `run()` 方法安排在一个独立的进程中进行调用。

```
from multiprocessing import Process
import os
def run_proc(name):
    print('Run child process %s (%s)...' % (name, os.getpid()))
if __name__ == '__main__':
    print('Parent process %s.' % os.getpid())
    p = Process(target=run_proc, args=('test',))
    print('Child process will start.')
    p.run()
    p.start()
    print(p.join())
    print('Child process end.')
```

输出：

```
Child process will start.
Run child process test (7696)...
Run child process test (2340)...
None
Child process end.
```

`p.run()` 和 `p.start()` 位于两个不同的进程中。

##### join([*timeout*])

如果 *timeout* 为 `None` (默认值)，该方法将阻塞，直到调用 `join()` 的进程终止。

If the optional argument *timeout* is `None` (the default), the method blocks until the process whose [`join()`](https://docs.python.org/3/library/multiprocessing.html#multiprocessing.Process.join) method is called terminates. If *timeout* is a positive number, it blocks at most *timeout* seconds. 
Note that the method returns `None` if its process terminates or if the method times out. Check the process’s [`exitcode`](https://docs.python.org/3/library/multiprocessing.html#multiprocessing.Process.exitcode) to determine if it terminated.

A process can be joined many times.

A process cannot join itself because this would cause a deadlock. It is an error to attempt尝试 to join a process before it has been started.

`join()` 方法可以等待子进程结束后再继续往下运行，通常用于进程间的同步。

##### pid

Return the process ID. Before the process is spawned, this will be `None`.

##### exitcode

The child’s exit code. 
This will be `None` if the process has not yet terminated. 
A negative value *-N* indicates that the child was terminated by signal *N*.

示例，启动一个子进程并等待其结束：

```
from multiprocessing import Process
import os

# 子进程要执行的代码
def run_proc(name):
    print('Run child process %s (%s)...' % (name, os.getpid()))

if __name__=='__main__':
    print('Parent process %s.' % os.getpid())
    p = Process(target=run_proc, args=('test',))
    print('Child process will start.')
    p.start()
    p.join()
    print('Child process end.')
```

执行结果如下：

```
Parent process 3504.
Child process will start.
Run child process test (2656)...
Child process end.
```

需要使用 `start()` 方法启动该实例。

#### Pool 类

可以创建一个进程池，该池将使用 Pool 类来执行提交给它的任务。

-   *class* multiprocessing.pool.Pool([*processes*[, *initializer*[, *initargs*[, *maxtasksperchild*[, *context*]]]]])

    A process pool object which controls a pool of worker processes to which jobs can be submitted. It supports asynchronous异步 results with timeouts and callbacks and has a parallel并行 map implementation.

    *processes* is the number of worker processes to use. If *processes* is `None` then the number returned by [`os.cpu_count()`](https://docs.python.org/3/library/os.html#os.cpu_count) is used.

    If *initializer* is not `None` then each worker process will call `initializer(*initargs)` when it starts.

    *maxtasksperchild* is the number of tasks a worker process can complete before it will exit and be replaced with a fresh worker process, to enable unused resources to be freed. The default *maxtasksperchild* is `None`, which means worker processes will live as long as the pool.

    *context* can be used to specify the context used for starting the worker processes. Usually a pool is created using the function `multiprocessing.Pool()` or the [`Pool()`](https://docs.python.org/3/library/multiprocessing.html#multiprocessing.pool.Pool) method of a context object. In both cases *context* is set appropriately.

    Note that the methods of the pool object should only be called by the process which created the pool.

    New in version 3.2: *maxtasksperchild*

    New in version 3.4: *context*

如果要启动大量的子进程，可以用进程池的方式批量创建子进程：

```
from multiprocessing import Pool
import os, time, random

def long_time_task(name):
    print('Run task %s (%s)...' % (name, os.getpid()))
    start = time.time()
    time.sleep(random.random() * 3)
    end = time.time()
    print('Task %s runs %0.2f seconds.' % (name, (end - start)))

if __name__=='__main__':
    print('Parent process %s.' % os.getpid())
    p = Pool(4)
    for i in range(5):
        p.apply_async(long_time_task, args=(i,))
    print('Waiting for all subprocesses done...')
    p.close()
    p.join()
    print('All subprocesses done.')
```

执行结果如下：

```
Parent process 669.
Waiting for all subprocesses done...
Run task 0 (671)...
Run task 1 (672)...
Run task 2 (673)...
Run task 3 (674)...
Task 2 runs 0.14 seconds.
Run task 4 (673)...
Task 1 runs 0.27 seconds.
Task 3 runs 0.86 seconds.
Task 0 runs 1.41 seconds.
Task 4 runs 1.91 seconds.
All subprocesses done.
```

注意：调用`close()`之后就不能继续添加新的`Process`了。所以 `close()` 要位于 `join` 之前。
`task 4` 需要等之前某个 `task` ，结束后才能执行。
*processes* is the number of worker processes to use. If *processes* is `None` then the number returned by [`os.cpu_count()`](https://docs.python.org/3/library/os.html#os.cpu_count) is used.
这不是操作系统的限制，而是 Pool 有意设计的限制。
所以，如果过改为：`p = Pool(5)` 就可以同时执行 `task 4` 了。 

##### apply

apply (*func*[, *args*[, *kwds*]]) 

Call *func* with arguments *args* and keyword arguments *kwds*. It blocks until the result is ready. Given this blocks, [`apply_async()`](https://docs.python.org/3/library/multiprocessing.html#multiprocessing.pool.Pool.apply_async) is better suited for performing work in parallel. Additionally, *func* is only executed in one of the workers of the pool.

使用参数args和关键字参数kwds调用func。 该方法会造成阻塞。
 Given this blocks， [`apply_async()`](https://docs.python.org/3/library/multiprocessing.html#multiprocessing.pool.Pool.apply_async) 更适合并行执行工作。
 此外，func 仅在池的一个  workers 中执行。

##### apply_async

apply_async (*func*[, *args*[, *kwds*[, *callback*[, *error_callback*]]]]) 

A variant of the [`apply()`](https://docs.python.org/3/library/multiprocessing.html#multiprocessing.pool.Pool.apply) method which returns a result object.

If *callback* is specified then it should be a callable which accepts a single argument. When the result becomes ready *callback* is applied to it, that is unless the call failed, in which case the *error_callback* is applied instead.

If *error_callback* is specified then it should be a callable which accepts a single argument. If the target function fails, then the *error_callback* is called with the exception instance.

Callbacks should complete immediately since otherwise the thread which handles the results will get blocked.

##### close()

Prevents any more tasks from being submitted to the pool. Once all the tasks have been completed the worker processes will exit.防止任何更多的任务被提交到池中。 一旦完成所有任务，工作进程将退出。

##### join()

Wait for the worker processes to exit. 
One must call [`close()`](https://docs.python.org/3/library/multiprocessing.html#multiprocessing.pool.Pool.close) or [`terminate()`](https://docs.python.org/3/library/multiprocessing.html#multiprocessing.pool.Pool.terminate) before using [`join()`](https://docs.python.org/3/library/multiprocessing.html#multiprocessing.pool.Pool.join).

## 2. subprocess 模块

`subprocess` — Subprocess management[¶](https://docs.python.org/3/library/subprocess.html#module-subprocess)

The `subprocess` module allows you to spawn生成 new processes, connect to their input/output/error pipes, and obtain获得 their return codes. 
子进程模块允许您生成新进程，连接到输入/输出/错误管道，并获取其返回代码。

在创建子进程后，还需要控制子进程的输入和输出。
`subprocess` 模块可以非常方便地启动一个子进程，然后控制其输入和输出。

### subprocess.run()

-   subprocess.run(*args*, \*, *stdin=None*, *input=None*, *stdout=None*, *stderr=None*, *shell=False*, *cwd=None*, *timeout=None*, *check=False*, *encoding=None*, *errors=None*)

    Run the command described by *args*. Wait for command to complete, then return a [`CompletedProcess`](https://docs.python.org/3/library/subprocess.html#subprocess.CompletedProcess) instance.

-   subprocess.call(*args*, ***, *stdin=None*, *stdout=None*, *stderr=None*, *shell=False*, *cwd=None*, *timeout=None*)

    Run the command described by *args*. Wait for command to complete, then return the [`returncode`](https://docs.python.org/3/library/subprocess.html#subprocess.Popen.returncode) attribute.

下面的例子演示了如何在 Python 代码中运行命令`nslookup www.python.org`，这和命令行直接运行的效果是一样的：

```
import subprocess

print('$ nslookup www.python.org')
r = subprocess.call(['nslookup', 'www.python.org'])
print('Exit code:', r)
```

运行结果：

```
$ nslookup www.python.org
Server:        192.168.19.4
Address:    192.168.19.4#53

Non-authoritative answer:
www.python.org    canonical name = python.map.fastly.net.
Name:    python.map.fastly.net
Address: 199.27.79.223

Exit code: 0
```

如果子进程还需要输入，则可以通过`communicate()`方法输入：

```
import subprocess

print('$ nslookup')
p = subprocess.Popen(['nslookup'], stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
output, err = p.communicate(b'set q=mx\npython.org\nexit\n')
print(output.decode('utf-8')) #需要将命令窗口的编码方式修改为'utf-8'
print('Exit code:', p.returncode)
```

上面的代码相当于在命令行执行命令`nslookup`，然后手动输入：

```
set q=mx
python.org
exit
```

运行结果如下：

```
$ nslookup
Server:        192.168.19.4
Address:    192.168.19.4#53

Non-authoritative answer:
python.org    mail exchanger = 50 mail.python.org.

Authoritative answers can be found from:
mail.python.org    internet address = 82.94.164.166
mail.python.org    has AAAA address 2001:888:2000:d::a6


Exit code: 0
```

## 3. 进程间通信

17.2.1.3. Exchanging objects between processes[¶](https://docs.python.org/3/library/multiprocessing.html#exchanging-objects-between-processes)

17.2.2.2. Pipes and Queues[¶](https://docs.python.org/3/library/multiprocessing.html#pipes-and-queues)

When using multiple processes, one generally uses message passing for communication between processes and avoids having to use any synchronization同步 primitives like locks.

For passing messages : 

-   one can use [`Pipe()`](https://docs.python.org/3/library/multiprocessing.html#multiprocessing.Pipe) (for a connection between two processes) 
-   or a queue (which allows multiple producers and consumers).

### Pipe 管道

-   multiprocessing. Pipe([*duplex*]) [¶](https://docs.python.org/3/library/multiprocessing.html#multiprocessing.Pipe)

    Returns a pair `(conn1, conn2)` of [`Connection`](https://docs.python.org/3/library/multiprocessing.html#multiprocessing.Connection) objects representing the ends of a pipe.

    If *duplex* 双工 is `True` (the default) then the pipe is bidirectional双向. If *duplex* is `False` then the pipe is unidirectional: `conn1` can only be used for receiving messages and `conn2` can only be used for sending messages.

For example:

```
from multiprocessing import Process, Pipe

def f(conn):
    conn.send([42, None, 'hello'])
    conn.close()

if __name__ == '__main__':
    parent_conn, child_conn = Pipe()
    p = Process(target=f, args=(child_conn,))
    p.start()
    print(parent_conn.recv())   # prints "[42, None, 'hello']"
    p.join()
```

The two connection objects returned by [`Pipe()`](https://docs.python.org/3/library/multiprocessing.html#multiprocessing.Pipe) represent the two ends of the pipe. Each connection object has [`send()`](https://docs.python.org/3/library/multiprocessing.html#multiprocessing.Connection.send) and [`recv()`](https://docs.python.org/3/library/multiprocessing.html#multiprocessing.Connection.recv) methods (among others). Note that data in a pipe may become corrupted损坏 if two processes (or threads) try to read from or write to the *same* end of the pipe at the same time. Of course当然 there is no risk风险 of corruption损坏 from processes using different ends of the pipe at the same time.

### Queue 队列

-   *class* multiprocessing.Queue([*maxsize*])[¶](https://docs.python.org/3/library/multiprocessing.html#multiprocessing.Queue)

    Returns a process shared queue implemented using a pipe and a few locks/semaphores. When a process first puts an item on the queue a feeder thread is started which transfers objects from a buffer into the pipe.

    返回一个可在进程间共享的队列，队列由一个 pipe 和一些 locks/semaphores 实现。当某个进程首先将一个 item 放入队列时，会启动一个反馈线程，该线程间对象从缓冲区传输到 pipe。

    The usual [`queue.Empty`](https://docs.python.org/3/library/queue.html#queue.Empty) and [`queue.Full`](https://docs.python.org/3/library/queue.html#queue.Full) exceptions异常 from the standard library’s [`queue`](https://docs.python.org/3/library/queue.html#module-queue) module are raised to signal timeouts.

    [`Queue`](https://docs.python.org/3/library/multiprocessing.html#multiprocessing.Queue) implements all the methods of [`queue.Queue`](https://docs.python.org/3/library/queue.html#queue.Queue) except for [`task_done()`](https://docs.python.org/3/library/queue.html#queue.Queue.task_done) and [`join()`](https://docs.python.org/3/library/queue.html#queue.Queue.join).

The [`Queue`](https://docs.python.org/3/library/multiprocessing.html#multiprocessing.Queue) class is a near clone of [`queue.Queue`](https://docs.python.org/3/library/queue.html#queue.Queue). For example:

```
from multiprocessing import Process, Queue

def f(q):
    q.put([42, None, 'hello'])

if __name__ == '__main__':
    q = Queue()
    p = Process(target=f, args=(q,))
    p.start()
    print(q.get())    # prints "[42, None, 'hello']"
    p.join()
```

Queues are thread and process safe.

在父进程中创建两个子进程，一个往 `Queue` 里写数据，一个从 `Queue` 里读数据：

```
from multiprocessing import Process, Queue
import os, time, random

# 写数据进程执行的代码:
def write(q):
    print('Process to write: %s' % os.getpid())
    for value in ['A', 'B', 'C']:
        print('Put %s to queue...' % value)
        q.put(value)
        time.sleep(random.random())

# 读数据进程执行的代码:
def read(q):
    print('Process to read: %s' % os.getpid())
    while True:
        value = q.get(True) 
        print('Get %s from queue.' % value)

if __name__=='__main__':
    # 父进程创建Queue，并传给各个子进程：
    q = Queue()
    pw = Process(target=write, args=(q,))
    pr = Process(target=read, args=(q,))
    # 启动子进程pw，写入:
    pw.start()
    # 启动子进程pr，读取:
    pr.start()
    # 等待pw结束:
    pw.join()
    # pr进程里是死循环，无法等待其结束，只能强行终止:
    pr.terminate()
```

运行结果如下：

```
Process to write: 50563
Put A to queue...
Process to read: 50564
Get A from queue.
Put B to queue...
Get B from queue.
Put C to queue...
Get C from queue.
```

在Unix/Linux下，`multiprocessing`模块封装了`fork()`调用，使我们不需要关注`fork()`的细节。由于Windows没有`fork`调用，因此，`multiprocessing`需要“模拟”出`fork`的效果，父进程所有 Python 对象都必须通过pickle序列化再传到子进程去，所以，如果`multiprocessing`在Windows下调用失败了，要先考虑是不是pickle 失败了。

## 4. Thread 线程

多任务可以由多进程完成，也可以由一个进程内的多线程完成。
而进程是由若干线程组成的，一个进程至少有一个线程。

由于线程是操作系统直接支持的执行单元。
因此，高级语言通常都内置多线程的支持，Python 也如此。
并且，Python的线程是真正的Posix Thread，并不是模拟出来的线程。

线程是用于解耦无顺序依赖关系任务的技术（线程是一种使没有顺序关系的任务并发执行的技术）。线程可以用来提高程序的相应能力，在接受用户输入的同时在后台执行另外的操作。

多线程应用程序的主要挑战是协调线程间的数据贡献及其它资源。
因此，线程模块提供了许多用于同步的原函数 primitives : locks, events, condition variables, and semaphores.
虽然这些工具很强大，但是小的设计错误可能会导致难以重现的问题。
因此，

虽然这些工具很强大，但是较小的设计错误也可能导致难以重现的问题。 因此，任务协调的首选方法是将所有对一个资源的访问集中在单个线程中，然后使用 `queue` 模块为该线程提供来自其他线程的请求。 使用`Queue` 对象进行线程间通信和协调的应用程序更容易设计、更可读、更可靠。

### threading / _thread 

threading 和 _thread 都是用于处理多线程的模块。

17.1. `threading` 模块 — Thread-based parallelism[¶](https://docs.python.org/3/library/threading.html#module-threading) 
该模块在较低级别的 `_thread` 模块之上构建了更高级别的线程接口。
大多数情况下，只需要使用 `threading` 模块。

17.9. `_thread` 模块 — Low-level threading API[¶](https://docs.python.org/3/library/_thread.html#module-_thread)
该模块提供了用于处理多线程的 low-level primitives。线程 thread 也被称为  light-weight processes or tasks。 
multiple threads of control sharing their global data space.
For synchronization, simple locks (also called mutexes or binary semaphores) are provided.
The threading module provides an easier to use and higher-level threading API built on top of this module.

### threading

#### threading.current_thread()

Return the current [`Thread`](https://docs.python.org/3/library/threading.html#threading.Thread) object, corresponding to the caller’s thread of control. If the caller’s thread of control was not created through the [`threading`](https://docs.python.org/3/library/threading.html#module-threading) module, a dummy thread object with limited functionality is returned. 返回当前的 Thread 对象，对应于调用者的线程控制。 如果调用者的线程控制未通过`threading` 模块创建，则返回具有有限功能的虚拟线程对象。

#### threading.Thread 类

*class* threading.Thread(*group=None*, *target=None*, *name=None*, *args=()*, *kwargs={}*, \*, *daemon=None*)

This constructor should always be called with keyword arguments. Arguments are:

-   *group* should be `None`; reserved保留 for future extension扩展 when a `ThreadGroup` class is implemented.
-   *target* is the callable object to be invoked by the [`run()`](https://docs.python.org/3/library/threading.html#threading.Thread.run) method. Defaults to `None`, meaning nothing is called.
-   *name* is the thread name. By default, a unique name is constructed of the form “Thread-*N*” where *N* is a small decimal number.
-   *args* is the argument tuple for the target invocation. Defaults to `()`.
-   *kwargs* is a dictionary of keyword arguments for the target invocation. Defaults to `{}`.
-   If not `None`, *daemon* 守护进程 explicitly sets whether the thread is daemonic守护. If `None` (the default), the daemonic守护 property is inherited from the current thread.
-   If the subclass overrides the constructor, it must make sure to invoke the base class constructor (`Thread.__init__()`) before doing anything else to the thread.

Changed in version 3.3: Added the *daemon* argument.

##### start()方法

start()

Start the thread’s activity.

It must be called at most once per thread object. It arranges for the object’s [`run()`](https://docs.python.org/3/library/threading.html#threading.Thread.run) method to be invoked in a separate thread of control.

This method will raise a [`RuntimeError`](https://docs.python.org/3/library/exceptions.html#RuntimeError) if called more than once on the same thread object.

##### join() 方法

join(*timeout=None*)

Wait until the thread terminates. This blocks the calling thread until the thread whose [`join()`](https://docs.python.org/3/library/threading.html#threading.Thread.join) method is called terminates – either normally or through an unhandled exception – or until the optional timeout occurs.

When the *timeout* argument is present and not `None`, it should be a floating point number specifying a timeout for the operation in seconds (or fractions thereof). As [`join()`](https://docs.python.org/3/library/threading.html#threading.Thread.join) always returns `None`, you must call [`is_alive()`](https://docs.python.org/3/library/threading.html#threading.Thread.is_alive) after [`join()`](https://docs.python.org/3/library/threading.html#threading.Thread.join) to decide whether a timeout happened – if the thread is still alive, the [`join()`](https://docs.python.org/3/library/threading.html#threading.Thread.join) call timed out.

When the *timeout* argument is not present or `None`, the operation will block until the thread terminates.

A thread can be [`join()`](https://docs.python.org/3/library/threading.html#threading.Thread.join)ed many times.

[`join()`](https://docs.python.org/3/library/threading.html#threading.Thread.join) raises a [`RuntimeError`](https://docs.python.org/3/library/exceptions.html#RuntimeError) if an attempt is made to join the current thread as that would cause a deadlock. It is also an error to [`join()`](https://docs.python.org/3/library/threading.html#threading.Thread.join) a thread before it has been started and attempts to do so raise the same exception.

示例，启动一个线程就是把一个函数传入并创建`Thread`实例，然后调用`start()`开始执行：

```
import time, threading

# 新线程执行的代码:
def loop():
    print('thread %s is running...' % threading.current_thread().name)
    n = 0
    while n < 5:
        n = n + 1
        print('thread %s >>> %s' % (threading.current_thread().name, n))
        time.sleep(1)
    print('thread %s ended.' % threading.current_thread().name)

print('thread %s is running...' % threading.current_thread().name)
t = threading.Thread(target=loop, name='LoopThread')
t.start()
t.join() #等待LoopThread线程执行完后，再执行后面的代码
print('thread %s ended.' % threading.current_thread().name)
```

执行结果如下：

```
thread MainThread is running...
thread LoopThread is running...
thread LoopThread >>> 1
thread LoopThread >>> 2
thread LoopThread >>> 3
thread LoopThread >>> 4
thread LoopThread >>> 5
thread LoopThread ended.
thread MainThread ended.

```

所有进程都会默认启动一个主线程，利用该主线程又可启动新的线程。

新开一个线程用于 I/O 并行计算。

```
import threading, zipfile

class AsyncZip(threading.Thread):
    def __init__(self, infile, outfile):
        threading.Thread.__init__(self)
        self.infile = infile
        self.outfile = outfile

    def run(self):
        f = zipfile.ZipFile(self.outfile, 'w', zipfile.ZIP_DEFLATED)
        f.write(self.infile)
        f.close()
        print('Finished background zip of:', self.infile)

background = AsyncZip('mydata.txt', 'myarchive.zip')
background.start()
print('The main program continues to run in foreground.')

background.join()    # Wait for the background task to finish
print('Main program waited until background was done.')
```


### Lock

在不同的进程中，同一变量拥有独立的拷贝，互不影响。
对于同一进程下的不同线程，它们共享相同的内存空间。因此，内存中的变量可被任一线程修改。那么，当多个线程同时修改同一变量时，便会使得变量内容失控。这使得多线程编程模型较为复杂，易发生冲突，同时必须枷锁隔离，但又需要放置锁死。

#### threading.Lock()

-   *class*t hreading.Lock[¶](https://docs.python.org/3/library/threading.html#threading.Lock)

    The class implementing primitive lock objects. Once a thread has acquired a lock, subsequent attempts to acquire it block, until it is released; any thread may release it.

    Note that `Lock` is actually a **factory function** which returns an instance of the most efficient最有效的 version of the concrete具体的 Lock class that is supported by the platform.

##### acquire()

acquire(*blocking=True*, *timeout=-1*)

Acquire a lock, blocking or non-blocking.

When invoked with the *blocking* argument set to `True` (the default), block until the lock is unlocked, then set it to locked and return `True`.

When invoked with the *blocking* argument set to `False`, do not block. If a call with *blocking* set to `True` would block, return `False` immediately; otherwise, set the lock to locked and return `True`. 

When invoked with the floating-point *timeout* argument set to a positive value, block for at most the number of seconds specified by *timeout* and as long as the lock cannot be acquired. A *timeout* argument of `-1` specifies an unbounded wait. It is forbidden to specify a *timeout* when *blocking* is false.

The return value is `True` if the lock is acquired successfully, `False` if not (for example if the *timeout* expired).

Changed in version 3.2: The *timeout* parameter is new.

Changed in version 3.2: Lock acquires can now be interrupted by signals on POSIX.



##### release()

Release a lock. This can be called from any thread, not only the thread which has acquired the lock.

When the lock is locked, reset it to unlocked, and return. If any other threads are blocked waiting for the lock to become unlocked, allow exactly one of them to proceed.

When invoked on an unlocked lock, a [`RuntimeError`](https://docs.python.org/3/library/exceptions.html#RuntimeError) is raised.

There is no return value.

```
balance = 0
lock = threading.Lock()

def change_it(n):
    # 先存后取，结果应该为0:
    global balance
    balance = balance + n
    balance = balance - n

def run_thread(n):
    for i in range(100000):
        # 先要获取锁:
        lock.acquire()
        try:
            # 放心地改吧:
            change_it(n)
        finally:
            # 改完了一定要释放锁:
            lock.release()
t1 = threading.Thread(target=run_thread, args=(5,))
t2 = threading.Thread(target=run_thread, args=(8,))
t1.start()
t2.start()
t1.join()
t2.join()
print(balance)
```

当多个线程同时执行 `lock.acquire()` 时，只有一个线程能成功地获取锁，然后继续执行代码，其他线程就继续等待直到获得锁为止。

获得锁的线程用完后一定要释放锁，否则那些苦苦等待锁的线程将永远等待下去，成为死线程。所以我们用`try...finally`来确保锁一定会被释放。

锁的好处就是确保了某段关键代码只能由一个线程从头到尾完整地执行，坏处当然也很多，首先是阻止了多线程并发执行，包含锁的某段代码实际上只能以单线程模式执行，效率就大大地下降了。其次，由于可以存在多个锁，不同的线程持有不同的锁，并试图获取对方持有的锁时，可能会造成死锁，导致多个线程全部挂起，既不能执行，也无法结束，只能靠操作系统强制终止。

### Global Interpreter Lock

The mechanism used by the CPython interpreter to assure确保 that only one thread executes Python bytecode at a time. 

如果你不幸拥有一个多核CPU，你肯定在想，多核应该可以同时执行多个线程。
如果写一个死循环的话，会出现什么情况呢？
打开Mac OS X的 Activity Monitor，或者Windows的Task Manager，都可以监控某个进程的CPU使用率。

我们可以监控到一个死循环线程会100%占用一个CPU。
如果有两个死循环线程，在多核CPU中，可以监控到会占用200%的CPU，也就是占用两个CPU核心。

要想把N核CPU的核心全部跑满，就必须启动N个死循环线程。
试试用Python写个死循环：

```
import threading, multiprocessing

def loop():
    x = 0
    while True:
        x = x ^ 1

for i in range(multiprocessing.cpu_count()):
    t = threading.Thread(target=loop)
    t.start()
```

启动与CPU核心数量相同的N个线程，在4核CPU上可以监控到CPU占用率仅有102%，也就是仅使用了一核。

但是用C、C++或Java来改写相同的死循环，直接可以把全部核心跑满，4核就跑到400%，8核就跑到800%，为什么Python不行呢？

因为 Python 的线程虽然是真正的线程，但解释器执行代码时，有一个GIL锁：Global Interpreter Lock，任何Python线程执行前，必须先获得GIL锁，然后，每执行100条字节码，解释器就自动释放GIL锁，让别的线程有机会执行。这个GIL全局锁实际上把所有线程的执行代码都给上了锁，所以，多线程在Python中只能交替执行，即使100个线程跑在100核CPU上，也只能用到1个核。

因此要尽可能避免使用多线程，这与 Py 的线程库没有任何关系，完全是因为 Py 的实现，特别是 CPython。 GIL 强制要求 Python 只能在一个解释器进程中运行，即使有多个处理器也只能如此。这意味着，如果在程序中使用了线程，即使有多个处理器这个程序也不会运行的更快，因为它根本无法使用多个处理器。线程应用会串行运行，而且在很多情况下，甚至比没有使用线程时还慢得多。

GIL是Python解释器设计的历史遗留问题，通常我们用的解释器是官方实现的CPython，要真正利用多核，除非重写一个不带GIL的解释器。

所以，在Python中，可以使用多线程，但不要指望能有效利用多核。如果一定要通过多线程利用多核，那只能通过C扩展来实现，不过这样就失去了Python简单易用的特点。

不过，也不用过于担心，Python虽然不能利用多线程实现多核任务，但可以通过多进程实现多核任务。多个Python进程有各自独立的GIL锁，互不影响。

Python解释器由于设计时有GIL全局锁，导致了多线程无法利用多核。多线程的并发在Python中就是一个美丽的梦。



### ThreadLocal

17.1.1. Thread-Local Data[¶](https://docs.python.org/3/library/threading.html#thread-local-data)

Thread-local data is data whose values are thread specific特定. To manage thread-local data, just create an instance of [`local`](https://docs.python.org/3/library/threading.html#threading.local) (or a subclass) and store attributes on it:

```
mydata = threading.local()
mydata.x = 1
```

The instance’s values will be different for separate threads.

-   *class*t hreading.local 

    A class that represents thread-local data.For more details and extensive examples, see the documentation string of the`_threading_local` module.

在多线程环境下，每个线程都有自己的数据。一个线程使用自己的局部变量比使用全局变量好，因为局部变量只有线程自己能看见，不会影响其他线程，而全局变量的修改必须加锁。

但是局部变量也有问题，就是在函数调用的时候，传递起来很麻烦：

```
def process_student(name):
    std = Student(name)
    # std是局部变量，但是每个函数都要用它，因此必须传进去：
    do_task_1(std)
    do_task_2(std)

def do_task_1(std):
    do_subtask_1(std)
    do_subtask_2(std)

def do_task_2(std):
    do_subtask_1(std)
    do_subtask_2(std)
```

每个函数一层一层调用都这么传参数那还得了？用全局变量？也不行，因为每个线程处理不同的 `Student` 对象，不能共享。

如果用一个全局`dict`存放所有的`Student`对象，然后以`thread`自身作为`key`获得线程对应的`Student`对象如何？

```
global_dict = {}

def std_thread(name):
    std = Student(name)
    # 把std放到全局变量global_dict中：
    global_dict[threading.current_thread()] = std
    do_task_1()
    do_task_2()

def do_task_1():
    # 不传入std，而是根据当前线程查找：
    std = global_dict[threading.current_thread()]
    ...

def do_task_2():
    # 任何函数都可以查找出当前线程的std变量：
    std = global_dict[threading.current_thread()]
    ...
```

这种方式理论上是可行的，它最大的优点是消除了`std`对象在每层函数中的传递问题，但是，每个函数获取`std`的代码有点丑。

有没有更简单的方式？

`ThreadLocal` 应运而生，不用查找`dict`，`ThreadLocal`帮你自动做这件事：

```
import threading

# 创建全局ThreadLocal对象:
local_school = threading.local()

def process_student():
    # 获取当前线程关联的student:
    std = local_school.student
    print('Hello, %s (in %s)' % (std, threading.current_thread().name))

def process_thread(name):
    # 绑定ThreadLocal的student:
    local_school.student = name
    process_student()

t1 = threading.Thread(target= process_thread, args=('Alice',), name='Thread-A')
t2 = threading.Thread(target= process_thread, args=('Bob',), name='Thread-B')
t1.start()
t2.start()
t1.join()
t2.join()
```

执行结果：

```
Hello, Alice (in Thread-A)
Hello, Bob (in Thread-B)
```

全局变量`local_school`就是一个`ThreadLocal`对象，每个`Thread`对它都可以读写`student`属性，但互不影响。你可以把`local_school`看成全局变量，但每个属性如`local_school.student`都是线程的局部变量，可以任意读写而互不干扰，也不用管理锁的问题，`ThreadLocal`内部会处理。

可以理解为全局变量`local_school`是一个`dict`，不但可以用`local_school.student`，还可以绑定其他变量，如`local_school.teacher`等等。

`ThreadLocal` 最常用的地方就是为每个线程绑定一个数据库连接，HTTP请求，用户身份信息等，这样一个线程的所有调用到的处理函数都可以非常方便地访问这些资源。

一个 `ThreadLocal` 变量虽然是全局变量，但每个线程都只能读写自己线程的独立副本，互不干扰。`ThreadLocal`解决了参数在一个线程中各个函数之间互相传递的问题。

## 5. 进程 vs. 线程

我们介绍了多进程和多线程，这是实现多任务最常用的两种方式。现在，我们来讨论一下这两种方式的优缺点。

首先，要实现多任务，通常我们会设计Master-Worker模式，Master负责分配任务，Worker负责执行任务，因此，多任务环境下，通常是一个Master，多个Worker。

如果用多进程实现Master-Worker，主进程就是Master，其他进程就是Worker。

如果用多线程实现Master-Worker，主线程就是Master，其他线程就是Worker。

多进程模式最大的优点就是稳定性高，因为一个子进程崩溃了，不会影响主进程和其他子进程。（当然主进程挂了所有进程就全挂了，但是Master进程只负责分配任务，挂掉的概率低）著名的Apache最早就是采用多进程模式。

多进程模式的缺点是创建进程的代价大，在Unix/Linux系统下，用`fork`调用还行，在Windows下创建进程开销巨大。另外，操作系统能同时运行的进程数也是有限的，在内存和CPU的限制下，如果有几千个进程同时运行，操作系统连调度都会成问题。

多线程模式通常比多进程快一点，但是也快不到哪去，而且，多线程模式致命的缺点就是任何一个线程挂掉都可能直接造成整个进程崩溃，因为所有线程共享进程的内存。在Windows上，如果一个线程执行的代码出了问题，你经常可以看到这样的提示：“该程序执行了非法操作，即将关闭”，其实往往是某个线程出了问题，但是操作系统会强制结束整个进程。

在Windows下，多线程的效率比多进程要高，所以微软的IIS服务器默认采用多线程模式。由于多线程存在稳定性的问题，IIS的稳定性就不如Apache。为了缓解这个问题，IIS和Apache现在又有多进程+多线程的混合模式，真是把问题越搞越复杂。

### 线程切换

无论是多进程还是多线程，只要数量一多，效率肯定上不去，为什么呢？

我们打个比方，假设你不幸正在准备中考，每天晚上需要做语文、数学、英语、物理、化学这5科的作业，每项作业耗时1小时。

如果你先花1小时做语文作业，做完了，再花1小时做数学作业，这样，依次全部做完，一共花5小时，这种方式称为单任务模型，或者批处理任务模型。

假设你打算切换到多任务模型，可以先做1分钟语文，再切换到数学作业，做1分钟，再切换到英语，以此类推，只要切换速度足够快，这种方式就和单核CPU执行多任务是一样的了，以幼儿园小朋友的眼光来看，你就正在同时写5科作业。

但是，切换作业是有代价的，比如从语文切到数学，要先收拾桌子上的语文书本、钢笔（这叫保存现场），然后，打开数学课本、找出圆规直尺（这叫准备新环境），才能开始做数学作业。操作系统在切换进程或者线程时也是一样的，它需要先保存当前执行的现场环境（CPU寄存器状态、内存页等），然后，把新任务的执行环境准备好（恢复上次的寄存器状态，切换内存页等），才能开始执行。这个切换过程虽然很快，但是也需要耗费时间。如果有几千个任务同时进行，操作系统可能就主要忙着切换任务，根本没有多少时间去执行任务了，这种情况最常见的就是硬盘狂响，点窗口无反应，系统处于假死状态。

所以，多任务一旦多到一个限度，就会消耗掉系统所有的资源，结果效率急剧下降，所有任务都做不好。

### 计算密集型 vs. IO密集型

是否采用多任务的第二个考虑是任务的类型。我们可以把任务分为计算密集型和IO密集型。

计算密集型任务的特点是要进行大量的计算，消耗CPU资源，比如计算圆周率、对视频进行高清解码等等，全靠CPU的运算能力。这种计算密集型任务虽然也可以用多任务完成，但是任务越多，花在任务切换的时间就越多，CPU执行任务的效率就越低，所以，要最高效地利用CPU，计算密集型任务同时进行的数量应当等于CPU的核心数。

计算密集型任务由于主要消耗CPU资源，因此，代码运行效率至关重要。Python这样的脚本语言运行效率很低，完全不适合计算密集型任务。对于计算密集型任务，最好用C语言编写。

第二种任务的类型是IO密集型，涉及到网络、磁盘IO的任务都是IO密集型任务，这类任务的特点是CPU消耗很少，任务的大部分时间都在等待IO操作完成（因为IO的速度远远低于CPU和内存的速度）。对于IO密集型任务，任务越多，CPU效率越高，但也有一个限度。常见的大部分任务都是IO密集型任务，比如Web应用。

IO密集型任务执行期间，99%的时间都花在IO上，花在CPU上的时间很少，因此，用运行速度极快的C语言替换用Python这样运行速度极低的脚本语言，完全无法提升运行效率。对于IO密集型任务，最合适的语言就是开发效率最高（代码量最少）的语言，脚本语言是首选，C语言最差。

### 异步IO

考虑到CPU和IO之间巨大的速度差异，一个任务在执行的过程中大部分时间都在等待IO操作，单进程单线程模型会导致别的任务无法并行执行，因此，我们才需要多进程模型或者多线程模型来支持多任务并发执行。

现代操作系统对IO操作已经做了巨大的改进，最大的特点就是支持异步IO。如果充分利用操作系统提供的异步IO支持，就可以用单进程单线程模型来执行多任务，这种全新的模型称为**事件驱动模型**，Nginx就是支持异步IO的Web服务器，它在单核CPU上采用单进程模型就可以高效地支持多任务。在多核CPU上，可以运行多个进程（数量与CPU核心数相同），充分利用多核CPU。由于系统总的进程数量十分有限，因此操作系统调度非常高效。用异步IO编程模型来实现多任务是一个主要的趋势。

对应到Python语言，单进程单线程的异步编程模型称为**协程**，有了协程的支持，就可以基于事件驱动编写高效的多任务程序。我们会在后面讨论如何编写协程。

## 6. 分布式进程

在Thread和Process中，应当优选Process，因为Process更稳定，而且，Process可以分布到多台机器上，而Thread最多只能分布到同一台机器的多个CPU上。

Python的`multiprocessing`模块不但支持多进程，其中`managers`子模块还支持把多进程分布到多台机器上。一个服务进程可以作为调度者，将任务分布到其他多个进程中，依靠网络通信。由于`managers`模块封装很好，不必了解网络通信的细节，就可以很容易地编写分布式多进程程序。

举个例子：如果我们已经有一个通过`Queue`通信的多进程程序在同一台机器上运行，现在，由于处理任务的进程任务繁重，希望把发送任务的进程和处理任务的进程分布到两台机器上。怎么用分布式进程实现？

原有的`Queue`可以继续使用，但是，通过`managers`模块把`Queue`通过网络暴露出去，就可以让其他机器的进程访问`Queue`了。

我们先看服务进程，服务进程负责启动`Queue`，把`Queue`注册到网络上，然后往`Queue`里面写入任务：

```
# task_master.py

import random, time, queue
from multiprocessing.managers import BaseManager

# 发送任务的队列:
task_queue = queue.Queue()
# 接收结果的队列:
result_queue = queue.Queue()

# 从BaseManager继承的QueueManager:
class QueueManager(BaseManager):
    pass

# 把两个Queue都注册到网络上, callable参数关联了Queue对象:
QueueManager.register('get_task_queue', callable=lambda: task_queue)
QueueManager.register('get_result_queue', callable=lambda: result_queue)
# 绑定端口5000, 设置验证码'abc':
manager = QueueManager(address=('', 5000), authkey=b'abc')
# 启动Queue:
manager.start()
# 获得通过网络访问的Queue对象:
task = manager.get_task_queue()
result = manager.get_result_queue()
# 放几个任务进去:
for i in range(10):
    n = random.randint(0, 10000)
    print('Put task %d...' % n)
    task.put(n)
# 从result队列读取结果:
print('Try get results...')
for i in range(10):
    r = result.get(timeout=10)
    print('Result: %s' % r)
# 关闭:
manager.shutdown()
print('master exit.')
```

请注意，当我们在一台机器上写多进程程序时，创建的`Queue`可以直接拿来用，但是，在分布式多进程环境下，添加任务到`Queue`不可以直接对原始的`task_queue`进行操作，那样就绕过了`QueueManager`的封装，必须通过`manager.get_task_queue()`获得的`Queue`接口添加。

然后，在另一台机器上启动任务进程（本机上启动也可以）：

```
# task_worker.py

import time, sys, queue
from multiprocessing.managers import BaseManager

# 创建类似的QueueManager:
class QueueManager(BaseManager):
    pass

# 由于这个QueueManager只从网络上获取Queue，所以注册时只提供名字:
QueueManager.register('get_task_queue')
QueueManager.register('get_result_queue')

# 连接到服务器，也就是运行task_master.py的机器:
server_addr = '127.0.0.1'
print('Connect to server %s...' % server_addr)
# 端口和验证码注意保持与task_master.py设置的完全一致:
m = QueueManager(address=(server_addr, 5000), authkey=b'abc')
# 从网络连接:
m.connect()
# 获取Queue的对象:
task = m.get_task_queue()
result = m.get_result_queue()
# 从task队列取任务,并把结果写入result队列:
for i in range(10):
    try:
        n = task.get(timeout=1)
        print('run task %d * %d...' % (n, n))
        r = '%d * %d = %d' % (n, n, n*n)
        time.sleep(1)
        result.put(r)
    except Queue.Empty:
        print('task queue is empty.')
# 处理结束:
print('worker exit.')
```

任务进程要通过网络连接到服务进程，所以要指定服务进程的IP。

现在，可以试试分布式进程的工作效果了。先启动`task_master.py`服务进程：

```
$ python3 task_master.py 
Put task 3411...
Put task 1605...
Put task 1398...
Put task 4729...
Put task 5300...
Put task 7471...
Put task 68...
Put task 4219...
Put task 339...
Put task 7866...
Try get results...
```

`task_master.py`进程发送完任务后，开始等待`result`队列的结果。现在启动`task_worker.py`进程：

```
$ python3 task_worker.py
Connect to server 127.0.0.1...
run task 3411 * 3411...
run task 1605 * 1605...
run task 1398 * 1398...
run task 4729 * 4729...
run task 5300 * 5300...
run task 7471 * 7471...
run task 68 * 68...
run task 4219 * 4219...
run task 339 * 339...
run task 7866 * 7866...
worker exit.

```

`task_worker.py`进程结束，在`task_master.py`进程中会继续打印出结果：

```
Result: 3411 * 3411 = 11634921
Result: 1605 * 1605 = 2576025
Result: 1398 * 1398 = 1954404
Result: 4729 * 4729 = 22363441
Result: 5300 * 5300 = 28090000
Result: 7471 * 7471 = 55815841
Result: 68 * 68 = 4624
Result: 4219 * 4219 = 17799961
Result: 339 * 339 = 114921
Result: 7866 * 7866 = 61873956

```

这个简单的Master/Worker模型有什么用？其实这就是一个简单但真正的分布式计算，把代码稍加改造，启动多个worker，就可以把任务分布到几台甚至几十台机器上，比如把计算`n*n`的代码换成发送邮件，就实现了邮件队列的异步发送。

Queue对象存储在哪？注意到`task_worker.py`中根本没有创建Queue的代码，所以，Queue对象存储在`task_master.py`进程中：

![task_master_worker](http://www.liaoxuefeng.com/files/attachments/001431929951799fda95088b12c48e2bd44b9157b8050c9000/l)

而`Queue`之所以能通过网络访问，就是通过`QueueManager`实现的。由于`QueueManager`管理的不止一个`Queue`，所以，要给每个`Queue`的网络调用接口起个名字，比如`get_task_queue`。

`authkey`有什么用？这是为了保证两台机器正常通信，不被其他机器恶意干扰。如果`task_worker.py`的`authkey`和`task_master.py`的`authkey`不一致，肯定连接不上。

Python的分布式进程接口简单，封装良好，适合需要把繁重任务分布到多台机器的环境下。

注意Queue的作用是用来传递任务和接收结果，每个任务的描述数据量要尽量小。比如发送一个处理日志文件的任务，就不要发送几百兆的日志文件本身，而是发送日志文件存放的完整路径，由Worker进程再去共享的磁盘上读取文件。





