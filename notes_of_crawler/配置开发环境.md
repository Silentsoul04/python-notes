#  开发环境配置
> GitHub@[orca-j35](https://github.com/orca-j35)，所有笔记均托管于 [python_notes](https://github.com/orca-j35/python_notes) 仓库

## 请求库

请求库用于实现 HTTP 请求操作，其作用是模拟浏览器向服务器发出请求操作。

### Requests

相关资源:

- PyPI: https://pypi.org/project/requests/
- GitHub: https://github.com/kennethreitz/requests
- Docs-EN: http://docs.python-requests.org/en/master/
- Docs-CN: http://docs.python-requests.org/zh_CN/latest/

安装:

```shell
conda install requests
```

### Aiohttp

requests 属于阻塞式 HTTP 请求库，当我们通过 requests 发出请求后，只有等服务器响应之后才会继续执行后续代码。

aiohttp 属于异步 HTTP Client/Server，Python3.5 中引入了 `async/await` 关键字，使得回调的写法更加直观和人性化。aiohttp 的异步操作借助于 `async/await` 关键字，使得写法变得更加简洁，架构更加清晰。

相关资源:

- Home: https://aiohttp.readthedocs.io
- PyPI: https://pypi.org/project/aiohttp/
- GitHub: https://github.com/aio-libs/aiohttp
- Docs-EN: https://aiohttp.readthedocs.io/en/stable/

aiohttp 官方还推荐 [cchardet](https://aiohttp.readthedocs.io/en/stable/glossary.html#term-cchardet)(字符编码检测库)和 [aiodns](https://aiohttp.readthedocs.io/en/stable/glossary.html#term-aiodns)(异步 DNS 解析库):

- cchardet: cChardet is high speed universal character encoding detector - binding to charsetdetect.

  https://pypi.org/project/cchardet/

  https://github.com/PyYoshi/cChardet

- chardet: The Universal Character Encoding Detector

  https://pypi.org/project/chardet/

- aiodns: DNS resolver for asyncio.

  https://pypi.org/project/aiodns

  https://github.com/saghul/aiodns

安装:

```shell
conda install aiohttp
conda install -c conda-forge cchardet
# conda和anaconda.org中都没有aiodns包
```

### Selenium

Selenium 是一个自动化测试工具，利用它可以驱动浏览器执行特定的动作，如点击、下拉等等。对于一些 JavaScript 渲染的页面来说，此种抓取方式非常有效。Selenium 需配合浏览器使用。

相关资源:

- Home: https://www.seleniumhq.org/
- PyPI: https://pypi.org/project/selenium/
- GitHub: https://github.com/SeleniumHQ/Selenium
- Github-Python: https://github.com/SeleniumHQ/selenium/tree/master/py
- Docs-Python-EN: https://seleniumhq.github.io/selenium/docs/api/py/index.html
- Docs-API-Python-EN: https://seleniumhq.github.io/selenium/docs/api/py/api.html
- Docs-EN(非官方): https://selenium-python.readthedocs.io/
- Docs-CN(非官方): https://selenium-python-zh.readthedocs.io/en/latest/

安装:

```shell
conda install selenium
```

## WebDriver

### ChromeDriver

ChromeDriver 是 WebDriver 用于驱动 Chrome 浏览器，Selenium 可利用 ChromeDriver 来驱动 Chrome 进行网页抓取。

ChromeDriver 和 Chrome 的版本号需相互匹配才能正常工作。

相关资源:

- Home: http://chromedriver.chromium.org/
- Downloads: http://chromedriver.chromium.org/downloads
- Chromedriver-Storage: https://chromedriver.storage.googleapis.com/index.html

在 [Downloads](http://chromedriver.chromium.org/downloads) 中下载与本机 Chrome 版本匹配 ChromeDriver，然后将 `chromedriver.exe` 置于 conda 环境的 `\Scripts` 文件夹中即可(如，`~\Anaconda3\envs\spider\Scripts`)，也可在环境变量中单独为 `chromedriver.exe` 配置路径。

在 Anaconda Prompt 中执行 ChromeDriver，如果获得类似输出，则证明 ChromeDriver 配置成功:

```shell
(spider) C:\WINDOWS\system32>chromedriver
Starting ChromeDriver 2.46.628402 (536cd7adbad73a3783fdc2cab92ab2ba7ec361e1) on port 9515
Only local connections are allowed.
Please protect ports used by ChromeDriver and related test frameworks to prevent access by malicious code.
```

还可利用 Python 脚本来测试 ChromeDriver 是否配置成功，代码如下:

```python
from selenium import webdriver
browser = webdriver.Chrome()
```

如果在运行上述代码后会弹出一个空白的 Chrome 浏览器，则证明 ChromeDriver 配置成功。

### GeckoDriver

GeckoDriver 是 WebDriver 用于驱动 Firefox 浏览器，Selenium 可利用 GeckoDriver 来驱动 Firefox 进行网页抓取。

相关资源:

- Home: https://github.com/mozilla/geckodriver
- Downloads: https://github.com/mozilla/geckodriver/releases
- Docs-EN: https://firefox-source-docs.mozilla.org/testing/geckodriver/

在 [Downloads](https://github.com/mozilla/geckodriver/releases) 中获取 GeckoDriver，然后将 `geckodriver.exe` 置于 conda 环境的 `\Scripts` 文件夹中即可(如，`~\Anaconda3\envs\spider\Scripts`)，也可在环境变量中单独为 `geckodriver.exe` 配置路径。

在 Anaconda Prompt 中执行 geckodriver，如果没有任何异常提示，则证明 GeckoDriver 配置成功:

```
(spider) C:\WINDOWS\system32>geckodriver
1552266734377   geckodriver     INFO    geckodriver 0.21.0
1552266734386   geckodriver     INFO    Listening on 127.0.0.1:4444
```

还可利用 Python 脚本来测试 GeckoDriver 是否配置成功，代码如下:

```python
from selenium import webdriver
browser = webdriver.Firefox()
```

如果在运行上述代码后会弹出一个空白的 Firefox 浏览器，则证明 GeckoDriver 配置成功。

## Headless Browser

### PhantomJS

利用 Selenium 通过 Chrome (或 Firefox )来抓取网页时，需要浏览器保持打开状态，并且在爬取网页的过程中浏览器可能会一直动来动去。如果不希望浏览器保持打开状态，可使用 headless 浏览器(如 PhantomJS)，新版本的 Chrome 和 Firefox 也支持 headless 模式。

PhantomJS 是使用 JavaScript 的 headless Web 浏览器，原生支持多种 Web 标准，如 DOM 操作、CSS 选择器、JSON、Canvas、SVG，但是现已停止开发。

注意:新版本的 Selenium 已不再支持 PhantomJS，需改用 Chrome 或 Firefox 的 headless 模式。

相关资源:

- Home: http://phantomjs.org/
- GitHub: https://github.com/ariya/phantomjs
- Downloads: http://phantomjs.org/download.html
- Docs-EN: http://phantomjs.org/documentation/
- [Quick Start with PhantomJS](http://phantomjs.org/quick-start.html)
- [Command Line Interface](http://phantomjs.org/api/command-line.html)

在 [Downloads](http://phantomjs.org/download.html) 中获取 PhantomJS，然后将下载包中的 `phantomjs.exe` 置于 conda 环境的 `\Scripts` 文件夹中即可(如，`~\Anaconda3\envs\spider\Scripts`)，也可在环境变量中单独为 `phantomjs.exe` 配置路径。

在 Anaconda Prompt 中执行 phantomjs，如果获得类似输出，则证明 PhantomJS 配置成功:

```python
(spider) C:\Users\iwhal>phantomjs -v
2.1.1
# 键入phantomjs后，会进入phantomjs的命令行模式
(spider) C:\Users\iwhal>phantomjs
phantomjs>
```

还可利用 Python 脚本来测试 PhantomJS 是否配置成功，代码如下:

```python
from selenium import webdriver
browser = webdriver.PhantomJS()
browser.get('https://cn.bing.com/')
print(browser.current_url)
'''Out:
C:\Users\iwhal\Anaconda3\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py:49: UserWarning: Selenium support for PhantomJS has been deprecated, please use headless versions of Chrome or Firefox instead
  warnings.warn('Selenium support for PhantomJS has been deprecated, please use headless '
https://cn.bing.com/
'''
```

以上输出表示配置成功，但 Selenium 会提示已不再支持 PhantomJS。

## 解析库

解析库用于从抓取到的网页中提取信息，如 LXML、BeautifulSoup、PyQuery 等等。这些库提供了非常强大的解析方法，如 XPath 解析、CSS 选择器解析等等。

### LXML

> The lxml XML toolkit is a Pythonic binding for the C libraries [libxml2](http://xmlsoft.org/) and [libxslt](http://xmlsoft.org/XSLT/). 

lxml 用于在 Python 语言环境中处理 XML 和 HTML，支持 XPath 解析方式，BeautifulSoup、Scrapy 框架均会用到此库。

相关资源:

- Home: https://lxml.de/
- PyPI: https://pypi.org/project/lxml/
- GitHub: https://github.com/lxml/lxml
- Docs-EN: https://lxml.de/index.html#documentation

安装:

```shell
conda install lxml
```

Tips: 如果在使用 `pip` 安装时，如果提示缺少依赖库(如 libxml2)，则可采用 wheel 方式安装。在 https://www.lfd.uci.edu/~gohlke/pythonlibs/#lxml 中下载需要的版本(如 Win64、Python3.6 需选择 lxml‑3.8.0‑cp36‑cp36m‑win_amd64.whl)，然后在本地安装 `.whl` 文件即可。

### BeautifulSoup

[Beautiful Soup](http://www.crummy.com/software/BeautifulSoup/) 是一个可以从 HTML 或 XML 文件中提取数据的 Python 库。它能够通过你喜欢的转换器实现惯用的文档导航、查找、修改文档的方式。Beautiful Soup 会帮你节省数小时甚至数天的工作时。

Beautiful Soup 支持 Python 标准库中的 HTML [解析器](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#id9)，同时还支持一些第三方的解析器(如 [lxml](http://lxml.de/))。详见 https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#id9

相关资源:

- Home: https://www.crummy.com/software/BeautifulSoup/
- PyPI: https://pypi.org/project/beautifulsoup4/
- Docs-EN: https://www.crummy.com/software/BeautifulSoup/bs4/doc/
- Docs-CN: https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/

安装:

```shell
conda install beautifulsoup4
```

执行以下代码可验证是否安装成功:

```python
from bs4 import BeautifulSoup
soup = BeautifulSoup('<p>Hello</p>', 'lxml')
print(soup.p.string) #> Hello
```

注意，安装时和导入时使用的库名称并不一定相同。比如在安装BeautifulSoup4 时，使用的名称是 `beautifulsoup4`；而在导入时，使用的名称却是 `bs4` (路径为 `~\Anaconda3\envs\spider\Lib\site-packages\bs4`)。

### PyQuery

> a jquery-like library for python
>
> pyquery allows you to make jquery queries on xml documents. The API is as much as possible the similar to jquery. pyquery uses lxml for fast xml and html manipulation.

相关资源:

- PyPI: https://pypi.org/project/pyquery/
- GitHub: https://github.com/gawel/pyquery
- Docs-EN: https://pyquery.readthedocs.io/en/latest/

安装:

```python
conda install pyquery
```

## Web 库

在编写爬虫的过程中，我们会用 Web 服务来搭建一些 API 接口。如, 利用 Flask+Redis 维护动态代理池和 Cookies 池。

### Flask

Flask 是一个轻量级的 [WSGI](https://wsgi.readthedocs.io/) Web 应用框架。

相关资源:

- Home: http://flask.pocoo.org/
- Website: https://www.palletsprojects.com/p/flask/
- PyPI: https://pypi.org/project/Flask/
- GitHub: https://github.com/pallets/flask
- Docs-EN: http://flask.pocoo.org/docs/1.0/
- Docs-CN:  http://docs.jinkan.org/docs/flask/(旧版文档)

安装:

```shell
pip install -U Flask
conda install Flask
```

安装测试，代码如下(详细解释 http://flask.pocoo.org/docs/1.0/quickstart/):

```python
from flask import Flask
app = Flask(__name__)

@app.route('/')
def hello_world():
    return 'Hello, World!'
```

在命令行中键入以下代码:

```shell
(spider) C:\Users\iwhal\Desktop\PyTest>set FLASK_APP=hello.py

(spider) C:\Users\iwhal\Desktop\PyTest>flask run
 * Serving Flask app "hello.py"
 * Environment: production
   WARNING: Do not use the development server in a production environment.
   Use a production WSGI server instead.
 * Debug mode: off
C:\Users\iwhal\Desktop\PyTest\hello.py:18: Warning: Silently ignoring app.run() because the application is run from the flask command line executable.  Consider putting app.run() behind an if __name__ == "__main__" guard to silence this warning.
  app.run()
 * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)
```

此时，在 http://127.0.0.1:5000/ 中可看到 `Hello, World!`，则证明 Flask 工作正常。

### Tornado

Tornado 是一个支持异步的 Web 框架，通过使用非阻塞 I/O 流，它可以支撑成千上万的开放连接，效率非常高。

相关资源:

- PyPI: https://pypi.org/project/tornado/
- GitHub: https://github.com/tornadoweb/tornado
- Docs-EN: http://www.tornadoweb.org/en/stable/

安装:

```shell
pip install tornado
conda install tornado
```

安装测试，代码如下:

```python
import tornado.ioloop
import tornado.web

class MainHandler(tornado.web.RequestHandler):
    def get(self):
        self.write("Hello, world")

def make_app():
    return tornado.web.Application([
        (r"/", MainHandler),
    ])

if __name__ == "__main__":
    app = make_app()
    app.listen(8888)
    tornado.ioloop.IOLoop.current().start()
```

此时，在 http://127.0.0.1:8888/ 中可看到 `Hello, World!`，则证明 Tornado 工作正常。

## App 爬取库

除了 Web 网页，还可利用爬虫抓取 App 数据。App 在获取数据时，



除了 Web 网页，爬虫也可以对 APP 的数据进行抓取，APP 中的页面要加载出来，首先需要获取数据，那么这些数据一般是通过请求服务器的接口来获取的，由于 APP 端没有像浏览器一样的开发者工具直接比较直观地看到后台的请求，所以对 APP 来说，它的数据抓取主要用到一些抓包技术。













