#  开发环境配置
> GitHub@[orca-j35](https://github.com/orca-j35)，所有笔记均托管于 [python_notes](https://github.com/orca-j35/python_notes) 仓库

## 请求库

请求库用于实现 HTTP 请求操作，其作用是模拟浏览器向服务器发出请求操作。

### Aiohttp

requests 属于阻塞式 HTTP 请求库，当我们通过 requests 发出请求后，只有等服务器响应之后才会继续执行后续代码。

aiohttp 属于异步 HTTP Client/Server，Python3.5 中引入了 `async/await` 关键字，使得回调的写法更加直观和人性化。aiohttp 的异步操作借助于 `async/await` 关键字，使得写法变得更加简洁，架构更加清晰。

相关资源:

- Home: https://aiohttp.readthedocs.io
- PyPI: https://pypi.org/project/aiohttp/
- GitHub: https://github.com/aio-libs/aiohttp
- Docs-EN: https://aiohttp.readthedocs.io/en/stable/

aiohttp 官方还推荐 [cchardet](https://aiohttp.readthedocs.io/en/stable/glossary.html#term-cchardet)(字符编码检测库)和 [aiodns](https://aiohttp.readthedocs.io/en/stable/glossary.html#term-aiodns)(异步 DNS 解析库):

- cchardet: cChardet is high speed universal character encoding detector - binding to charsetdetect.

  https://pypi.org/project/cchardet/

  https://github.com/PyYoshi/cChardet

- chardet: The Universal Character Encoding Detector

  https://pypi.org/project/chardet/

- aiodns: DNS resolver for asyncio.

  https://pypi.org/project/aiodns

  https://github.com/saghul/aiodns

安装:

```shell
conda install aiohttp
conda install -c conda-forge cchardet
# conda和anaconda.org中都没有aiodns包
```

如果需要用到 DNS 异步解析的话，还可以考虑 [Pycares](https://pypi.org/project/pycares/)。

### Selenium

Selenium 是一个自动化测试工具，利用它可以驱动浏览器执行特定的动作，如点击、下拉等等。对于一些 JavaScript 渲染的页面来说，此种抓取方式非常有效。Selenium 需配合浏览器使用。

相关资源:

- Home: https://www.seleniumhq.org/
- PyPI: https://pypi.org/project/selenium/
- GitHub: https://github.com/SeleniumHQ/Selenium
- Github-Python: https://github.com/SeleniumHQ/selenium/tree/master/py
- Docs-Python-EN: https://seleniumhq.github.io/selenium/docs/api/py/index.html
- Docs-API-Python-EN: https://seleniumhq.github.io/selenium/docs/api/py/api.html
- Docs-EN(非官方): https://selenium-python.readthedocs.io/
- Docs-CN(非官方): https://selenium-python-zh.readthedocs.io/en/latest/
- 教程 - [测试教程网](http://www.testclass.net/all): http://www.testclass.net/selenium_python

安装:

```shell
conda install selenium
```

## WebDriver

### ChromeDriver

ChromeDriver 是 WebDriver 用于驱动 Chrome 浏览器，Selenium 可利用 ChromeDriver 来驱动 Chrome 进行网页抓取。

ChromeDriver 和 Chrome 的版本号需相互匹配才能正常工作。

相关资源:

- Home: http://chromedriver.chromium.org/
- Downloads: http://chromedriver.chromium.org/downloads
- Chromedriver-Storage: https://chromedriver.storage.googleapis.com/index.html

在 [Downloads](http://chromedriver.chromium.org/downloads) 中下载与本机 Chrome 版本匹配 ChromeDriver，然后将 `chromedriver.exe` 置于 conda 环境的 `\Scripts` 文件夹中即可(如，`~\Anaconda3\envs\spider\Scripts`)，也可在环境变量中单独为 `chromedriver.exe` 配置路径。

在 Anaconda Prompt 中执行 ChromeDriver，如果获得类似输出，则证明 ChromeDriver 配置成功:

```shell
(spider) C:\WINDOWS\system32>chromedriver
Starting ChromeDriver 2.46.628402 (536cd7adbad73a3783fdc2cab92ab2ba7ec361e1) on port 9515
Only local connections are allowed.
Please protect ports used by ChromeDriver and related test frameworks to prevent access by malicious code.
```

还可利用 Python 脚本来测试 ChromeDriver 是否配置成功，代码如下:

```python
from selenium import webdriver
browser = webdriver.Chrome()
```

如果在运行上述代码后会弹出一个空白的 Chrome 浏览器，则证明 ChromeDriver 配置成功。

### GeckoDriver

GeckoDriver 是 WebDriver 用于驱动 Firefox 浏览器，Selenium 可利用 GeckoDriver 来驱动 Firefox 进行网页抓取。

相关资源:

- Home: https://github.com/mozilla/geckodriver
- Downloads: https://github.com/mozilla/geckodriver/releases
- Docs-EN: https://firefox-source-docs.mozilla.org/testing/geckodriver/

在 [Downloads](https://github.com/mozilla/geckodriver/releases) 中获取 GeckoDriver，然后将 `geckodriver.exe` 置于 conda 环境的 `\Scripts` 文件夹中即可(如，`~\Anaconda3\envs\spider\Scripts`)，也可在环境变量中单独为 `geckodriver.exe` 配置路径。

在 Anaconda Prompt 中执行 geckodriver，如果没有任何异常提示，则证明 GeckoDriver 配置成功:

```
(spider) C:\WINDOWS\system32>geckodriver
1552266734377   geckodriver     INFO    geckodriver 0.21.0
1552266734386   geckodriver     INFO    Listening on 127.0.0.1:4444
```

还可利用 Python 脚本来测试 GeckoDriver 是否配置成功，代码如下:

```python
from selenium import webdriver
browser = webdriver.Firefox()
```

如果在运行上述代码后会弹出一个空白的 Firefox 浏览器，则证明 GeckoDriver 配置成功。

## Headless Browser

### PhantomJS

利用 Selenium 通过 Chrome (或 Firefox )来抓取网页时，需要浏览器保持打开状态，并且在爬取网页的过程中浏览器可能会一直动来动去。如果不希望浏览器保持打开状态，可使用 headless 浏览器(如 PhantomJS)，新版本的 Chrome 和 Firefox 也支持 headless 模式。

PhantomJS 是使用 JavaScript 的 headless Web 浏览器，原生支持多种 Web 标准，如 DOM 操作、CSS 选择器、JSON、Canvas、SVG，但是现已停止开发。

注意:新版本的 Selenium 已不再支持 PhantomJS，需改用 Chrome 或 Firefox 的 headless 模式。

相关资源:

- Home: http://phantomjs.org/
- GitHub: https://github.com/ariya/phantomjs
- Downloads: http://phantomjs.org/download.html
- Docs-EN: http://phantomjs.org/documentation/
- [Quick Start with PhantomJS](http://phantomjs.org/quick-start.html)
- [Command Line Interface](http://phantomjs.org/api/command-line.html)

在 [Downloads](http://phantomjs.org/download.html) 中获取 PhantomJS，然后将下载包中的 `phantomjs.exe` 置于 conda 环境的 `\Scripts` 文件夹中即可(如，`~\Anaconda3\envs\spider\Scripts`)，也可在环境变量中单独为 `phantomjs.exe` 配置路径。

在 Anaconda Prompt 中执行 phantomjs，如果获得类似输出，则证明 PhantomJS 配置成功:

```python
(spider) C:\Users\iwhal>phantomjs -v
2.1.1
# 键入phantomjs后，会进入phantomjs的命令行模式
(spider) C:\Users\iwhal>phantomjs
phantomjs>
```

还可利用 Python 脚本来测试 PhantomJS 是否配置成功，代码如下:

```python
from selenium import webdriver
browser = webdriver.PhantomJS()
browser.get('https://cn.bing.com/')
print(browser.current_url)
'''Out:
C:\Users\iwhal\Anaconda3\lib\site-packages\selenium\webdriver\phantomjs\webdriver.py:49: UserWarning: Selenium support for PhantomJS has been deprecated, please use headless versions of Chrome or Firefox instead
  warnings.warn('Selenium support for PhantomJS has been deprecated, please use headless '
https://cn.bing.com/
'''
```

以上输出表示配置成功，但 Selenium 会提示已不再支持 PhantomJS。

## Web 库

在编写爬虫的过程中，我们会用 Web 服务来搭建一些 API 接口。如, 利用 Flask+Redis 维护动态代理池和 Cookies 池。

### Flask

Flask 是一个轻量级的 [WSGI](https://wsgi.readthedocs.io/) Web 应用框架。

相关资源:

- Home: http://flask.pocoo.org/
- Website: https://www.palletsprojects.com/p/flask/
- PyPI: https://pypi.org/project/Flask/
- GitHub: https://github.com/pallets/flask
- Docs-EN: http://flask.pocoo.org/docs/1.0/
- Docs-CN:  http://docs.jinkan.org/docs/flask/(旧版文档)

安装:

```shell
pip install -U Flask
conda install Flask
```

安装测试，代码如下(详细解释 http://flask.pocoo.org/docs/1.0/quickstart/):

```python
from flask import Flask
app = Flask(__name__)

@app.route('/')
def hello_world():
    return 'Hello, World!'
```

在命令行中键入以下代码:

```shell
(spider) C:\Users\iwhal\Desktop\PyTest>set FLASK_APP=hello.py

(spider) C:\Users\iwhal\Desktop\PyTest>flask run
 * Serving Flask app "hello.py"
 * Environment: production
   WARNING: Do not use the development server in a production environment.
   Use a production WSGI server instead.
 * Debug mode: off
C:\Users\iwhal\Desktop\PyTest\hello.py:18: Warning: Silently ignoring app.run() because the application is run from the flask command line executable.  Consider putting app.run() behind an if __name__ == "__main__" guard to silence this warning.
  app.run()
 * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)
```

此时，在 http://127.0.0.1:5000/ 中可看到 `Hello, World!`，则证明 Flask 工作正常。

### Tornado

Tornado 是一个支持异步的 Web 框架，通过使用非阻塞 I/O 流，它可以支撑成千上万的开放连接，效率非常高。

相关资源:

- PyPI: https://pypi.org/project/tornado/
- GitHub: https://github.com/tornadoweb/tornado
- Docs-EN: http://www.tornadoweb.org/en/stable/

安装:

```shell
pip install tornado
conda install tornado
```

安装测试，代码如下:

```python
import tornado.ioloop
import tornado.web

class MainHandler(tornado.web.RequestHandler):
    def get(self):
        self.write("Hello, world")

def make_app():
    return tornado.web.Application([
        (r"/", MainHandler),
    ])

if __name__ == "__main__":
    app = make_app()
    app.listen(8888)
    tornado.ioloop.IOLoop.current().start()
```

此时，在 http://127.0.0.1:8888/ 中可看到 `Hello, World!`，则证明 Tornado 工作正常。

## App 爬取库

爬虫不仅可用于抓取 Web 网页数据，还可抓取 App 数据。App 在获取数据时，通常会请求服务器的相应接口。与 Web 不同，在 App 端并没有像浏览器这样的开发工具(浏览器可以帮助我们直观的分析后台请求过程)，因此在抓取 App 数据时需要用到一些抓包技术。

本节介绍的抓包工具有 Charles, MitmProxy, MitmDump。对于一些简单的接口，可以先利用 Charles 或 MitmProxy 分析 App 的运行规律，然后用程序来模拟 App 的运行方式，从而实现数据抓取。对于复杂的接口，就需要用 MitmDump 和 Python 进行对接，从而实现对抓取到的请求和响应进行实时处理和保存。在进行大规模采集时，还需要使用 Appium 来对 App 进行自动化控制(如, 自动模拟 App 的点击、下拉等操作)。

### Charles Proxy

详见笔记﹝Charles Proxy.md﹞

### Wireshark

Wireshark（前称Ethereal）是一个[网络封包](https://baike.baidu.com/item/网络封包)分析软件。网络封包分析软件的功能是撷取网络封包，并尽可能显示出最为详细的网络封包资料。Wireshark使用WinPCAP作为接口，直接与网卡进行数据报文交换。

Home: <https://www.wireshark.org/>

### Fiddler

相关资源:

- Home: https://www.telerik.com/fiddler
- DOCS & SUPPORT: https://www.telerik.com/support/fiddler
- More Fiddler Tools

  - [FiddlerCore](https://www.telerik.com/fiddler/fiddlercore): .NET应用程序的流量查看和修改
  - [FiddlerCap](https://www.telerik.com/fiddler/fiddlercap): 捕获 Web 流量日志
  - [FiddlerCap with Firefox](https://www.telerik.com/fiddler/fiddlercap-with-firefox):使用 Firefox 捕获 Web 流量日志
- [Fiddler 抓包工具总结](https://www.cnblogs.com/yyhh/p/5140852.html)
- [Revisiting Fiddler and Win8+ Immersive applications](https://blogs.msdn.microsoft.com/fiddler/2011/12/10/revisiting-fiddler-and-win8-immersive-applications/)
- 教程 - [测试教程网](http://www.testclass.net/all): http://www.testclass.net/proxy_tools

重要的配置步骤:

- [为Windows 8配置Fiddler](http://docs.telerik.com/fiddler/Configure-Fiddler/Tasks/ConfigureFiddlerForWin8)
- [配置Fiddler以解密HTTPS流量](http://docs.telerik.com/fiddler/Configure-Fiddler/Tasks/DecryptHTTPS)
- [从IE或.NET监控到localhost的流量](http://docs.telerik.com/fiddler/Configure-Fiddler/Tasks/MonitorLocalTraffic)

### MitmProxy

MitmProxy 是一组工具集，包含以下内容:

- `mitmproxy` 是用于 HTTP 和 HTTPS 的交互式中间人代理(*man-in-the-middle* *proxy*)，并带有控制台接口，在 Windows 下没有该模块。
- `mitmdump` 是 `mitmproxy` 的命令行版本
- `mitmweb` 是一个基于 Web 接口的 `mitmproxy`
- `pathoc` and `pathod` are perverse HTTP client and server applications designed to let you craft almost any conceivable HTTP request, including ones that creatively violate the standards.

在原生 Windows 安装包中仅包含 `mitmdump.exe` 和 `mitmweb.exe`，并没有为 Windows 提供可用的 `mitmproxy` 模块。如果需要在 Windows 中使用 `mitmproxy`，则要使用 [WSL (Windows Subsystem for Linux)](https://docs.microsoft.com/en-us/windows/wsl/about)。

在 Windows 中可使用安装器或 `pip` (`conda` 中没有此包)来安装 MitmProxy，这里我比较推荐使用安装器进行安装。另外，安装器会自动在当前用户的环境变量中为 MitmProxy 添加路径，如 `C:\Program Files (x86)\mitmproxy\bin`。更多于安装相关的信息，请看 https://docs.mitmproxy.org/stable/overview-installation/

原生 Windows 安装包中并不包含 `pathoc.exe` 和 `pathod.exe`，可以在 https://mitmproxy.org/downloads/ 下载

相关资源

- Home: https://mitmproxy.org/
- PyPI: https://pypi.org/project/mitmproxy/
- GitHub: https://github.com/mitmproxy/mitmproxy
- Downloads: https://mitmproxy.org/downloads/
- Docs-stabel: https://docs.mitmproxy.org/stable/
- Docs-master: https://docs.mitmproxy.org/master/
- DockerHub: https://hub.docker.com/r/mitmproxy/mitmproxy

#### 安装证书

> 参考: 
>
> - https://docs.mitmproxy.org/stable/concepts-certificates/
> - 《Python3网络爬虫开发实战》-> 1.7 App爬取相关库的安装 ->  MitmProxy的安装

安装 mitmproxy 证书的原因与安装 Charles's  SSL 证书的原因类似，可参考﹝安装 Charles's  SSL 证书﹞中的类容。

安装 mitmproxy 证书的最简单的方法是使用 mitmproxy 内置的证书安装应用，使用方法如下：

1. 启动 mitmproxy，并未目标设备正确配置代理
2. 在目标设备的浏览器中访问 mitm.it
3. 在页面中点选当前设备对于的图标，然后下载并安装证书即可

#### CA and cert files

The files created by mitmproxy in the .mitmproxy directory are as follows:

| file name             | description                                                  |
| :-------------------- | :----------------------------------------------------------- |
| mitmproxy-ca.p12      |                                                              |
| mitmproxy-ca.pem      | The certificate **and the private key** in PEM format.       |
| mitmproxy-ca-cert.pem | The certificate in PEM format. Use this to distribute on most non-Windows platforms. |
| mitmproxy-ca-cert.p12 | The certificate in PKCS12 format. For use on Windows.        |
| mitmproxy-ca-cert.cer | Same file as .pem, but with an extension expected by some Android devices. |
| mitmproxy-dhparam.pem | PEM 格式的秘钥文件，用于增强 SSL 安全性                      |

### Appium

Appium 是一个开源的移动端自动化测试框架，用于自动化测试 iOS、Android、Windows 平台上的原生应用、移动 Web 应用和混合应用。

“原生应用”指那些用 iOS 、 Android 或者 Windows SDK 编写的应用。“移动 web 应用”是用移动端浏览器访问的应用（Appium 支持 iOS 上的 Safari 、Chrome 和 Android 上的内置浏览器）。“混合应用”带有一个 "webview" 的包装器——用来和 Web 内容交互的原生控件。

相关资源:

- Home: https://appium.io/
- GitHub: https://github.com/appium/appium
- Downloads: http://appium.io/downloads.html
- Docs: 
  - https://appium.io/documentation.html
  - https://github.com/appium/appium/tree/master/docs
  - docs-cn: https://appium.io/docs/cn/about-appium/intro/(中文版的内容比英文版旧)
  - [TesterHome Appium 中文文档小组](https://testerhome.com/appium-doc-cn)
  - doca-en: https://appium.io/docs/en/about-appium/intro/
- Appium-Python-Client:
  - PyPI: https://pypi.org/project/Appium-Python-Client/
  - GitHub: https://github.com/appium/python-client
- Appium Desktop: Appium Server and Inspector in Desktop GUIs for Mac, Windows, and Linux
  - GitHub: https://github.com/appium/appium-desktop
  - Releases: https://github.com/appium/appium-desktop/releases
- 教程 - [测试教程网](http://www.testclass.net/all): 
  - [appium新手入门](http://www.testclass.net/appium_base)
  - [Appium 简明教程](http://www.testclass.net/appium)

#### 配置 Android 环境

> 参考:
>
> - http://www.testclass.net/appium_base/appium-base-sdk
> - 《Python3网络爬虫开发实战》-> 1.7 App爬取相关库的安装 -> Appium的安装

Android 相关资源:

- https://developer.android.google.cn/studio
- https://developer.android.google.cn/studio/intro
- https://developers.google.cn/china/

## 爬虫框架

### PySpider

相关资源:

- PyPI: https://pypi.org/project/pyspider/
- GitHub: https://github.com/binux/pyspider
- Tutorial: http://docs.pyspider.org/en/latest/tutorial/
- Docs: http://docs.pyspider.org/

pyspider 目前只支持 Python 2.{6,7}, 3.{3,4,5,6}，conda 中搜不到 pyspider。pyspider 支持 JavaScript 渲染，渲染过程依赖于 PhantomJS。

```shell
pip install pyspider
```

因为 Python 3.7 已将 `async` 用作关键字，所以必须将以下文件中的 `async` 改为 `async_mode`，也可直接 clone [binux/pyspider](binux/pyspider) 然后替换这些文件。

- ~\Lib\site-packages\pyspider\run.py
- ~\Lib\site-packages\pyspider\fetcher\tornado_fetcher.py
- ~\Lib\site-packages\pyspider\webui\app.py

还需对 ~\Lib\site-packages\pyspider\webui\webdav.py 做如下修改: (注，目前  [binux/pyspider](binux/pyspider) 中尚未修改此处)——参考: https://segmentfault.com/q/1010000015429020?utm_source=tag-newest

```python
# 'domaincontroller': NeedAuthController(app),
'http_authenticator': {
    'HTTPAuthenticator':NeedAuthController(app),
},
```

如果还有别的安装问题，可参考以下 URL:

- https://www.cnblogs.com/kerbside/p/9630388.html
- https://blog.csdn.net/wyd117/article/details/84932858

安装测试，运行 pyspide:

```shell
(spider) C:\Users\iwhal>pyspider
c:\anaconda3\envs\spider\lib\site-packages\pyspider\libs\utils.py:196: FutureWarning: timeout is not supported on your platform.
  warnings.warn("timeout is not supported on your platform.", FutureWarning)
phantomjs fetcher running on port 25555
[I 190320 15:55:22 result_worker:49] result_worker starting...
[I 190320 15:55:23 processor:211] processor starting...
[I 190320 15:55:23 scheduler:647] scheduler starting...
[I 190320 15:55:23 scheduler:586] in 5m: new:0,success:0,retry:0,failed:0
[I 190320 15:55:23 tornado_fetcher:671] fetcher starting...
[I 190320 15:55:23 scheduler:782] scheduler.xmlrpc listening on 127.0.0.1:23333
[I 190320 15:55:23 app:74] webui running on 0.0.0.0:5000
```

此时如果可以通过 <http://localhost:5000/> 进入 pyspider dashboard 则表示配置成功。

### Scrapy

相关资源:

- Home: https://scrapy.org/
- PyPI: https://pypi.org/project/Scrapy/
- GitHub: https://github.com/scrapy/scrapy
- Docs-EN: 
  - https://docs.scrapy.org/en/latest/
  - https://scrapy.readthedocs.io/en/latest/index.html
- Docs-CN: https://scrapy-chs.readthedocs.io/zh_CN/latest/ (0.25 版)

建议使用 conda 安装，Scrapy 用纯 Python 编写，会依赖一些 Python 库。

```shell
conda install -c conda-forge scrapy
```

Scrapy 依赖的 Python 库：

- [lxml](http://lxml.de/), an efficient XML and HTML parser
- [parsel](https://pypi.python.org/pypi/parsel), an HTML/XML data extraction library written on top of lxml,
- [w3lib](https://pypi.python.org/pypi/w3lib), a multi-purpose helper for dealing with URLs and web page encodings
- [twisted](https://twistedmatrix.com/), an asynchronous networking framework
- [cryptography](https://cryptography.io/) and [pyOpenSSL](https://pypi.python.org/pypi/pyOpenSSL), to deal with various network-level security needs

如果使用 `pip` 安装 Scrapy，则可能需要手动安装上面这些库。具体过程可参考 《Python3网络爬虫开发实战》-> 1.8 爬虫框架的安装 -> Scrapy 的安装

如果在命令行中能够执行 scrapy，则说明安装成功。

### Scrapy-Splash

This library provides [Scrapy](https://github.com/scrapy/scrapy) and JavaScript integration using [Splash](https://github.com/scrapinghub/splash). 

Scrapy-Splash 是一个 Scrapy 中支持 JavaScript 渲染的工具，Scrapy-Splash 的安装分为两部分，一个是是 Splash 服务的安装，安装方式是通过 Docker，安装之后会启动一个 Splash 服务，我们可以通过它的接口来实现 JavaScript 页面的加载。另外一个是 ScrapySplash 的 Python 库的安装，安装之后即可在 Scrapy 中使用 Splash 服务。

#### scrapy-splash

相关资源:

- PyPI: https://pypi.org/project/scrapy-splash/
- GitHub: https://github.com/scrapy-plugins/scrapy-splash
- Docs: https://github.com/scrapy-plugins/scrapy-splash#configuration

安装:

Install scrapy-splash using pip:

```
$ pip install scrapy-splash
```

Scrapy-Splash uses [Splash](https://github.com/scrapinghub/splash) HTTP API, so you also need a Splash instance. Usually to install & run Splash, something like this is enough:

```
$ docker run -p 8050:8050 scrapinghub/splash
```

Check Splash [install docs](http://splash.readthedocs.org/en/latest/install.html) for more info.

#### splash

Scrapy-Splash 会使用 Splash 的 HTTP API 进行页面渲染，所以我们需要安装 Splash 来提供渲染服务，需要通过 Docker 来安装 splash，因此请确保已经正确安装了 Docker。

相关资源:

- GitHub: https://github.com/scrapinghub/splash
- Docs: https://splash.readthedocs.io/

安装:(https://splash.readthedocs.io/en/latest/install.html)

1. Install [Docker](http://docker.io/).

2. Pull the image:

   ```
   $ sudo docker pull scrapinghub/splash
   ```

   在 Windows 上的执行效果如下:

   ```shell
   (base) C:\WINDOWS\system32>docker pull scrapinghub/splash
   Using default tag: latest
   latest: Pulling from scrapinghub/splash
   7b722c1070cd: Pull complete
   5fbf74db61f1: Pull complete
   ed41cb72e5c9: Pull complete
   7ea47a67709e: Pull complete
   b9ea67282e79: Pull complete
   8d0589f2b410: Pull complete
   11f417145dc7: Pull complete
   14d670a8125e: Pull complete
   81d8bf1e3bdc: Pull complete
   Digest: sha256:ec1198946284ccadf6749ad60b58b2d2fd5574376857255342a913ec7c66cfc5
   Status: Downloaded newer image for scrapinghub/splash:latest
   ```

3. Start the container:

   ```
   $ sudo docker run -it -p 8050:8050 scrapinghub/splash
   ```

   在 Windows 上的执行效果如下:

   ```shell
   (base) C:\WINDOWS\system32>docker run -it -p 8050:8050 scrapinghub/splash
   2019-03-21 07:39:25+0000 [-] Log opened.
   2019-03-21 07:39:25.856452 [-] Splash version: 3.3.1
   2019-03-21 07:39:25.860093 [-] Qt 5.9.1, PyQt 5.9.2, WebKit 602.1, sip 4.19.4, Twisted 18.9.0, Lua 5.2
   2019-03-21 07:39:25.860413 [-] Python 3.5.2 (default, Nov 12 2018, 13:43:14) [GCC 5.4.0 20160609]
   2019-03-21 07:39:25.860706 [-] Open files limit: 1048576
   2019-03-21 07:39:25.860913 [-] Can't bump open files limit
   2019-03-21 07:39:25.972187 [-] Xvfb is started: ['Xvfb', ':1899201585', '-screen', '0', '1024x768x24', '-nolisten', 'tcp']
   QStandardPaths: XDG_RUNTIME_DIR not set, defaulting to '/tmp/runtime-root'
   2019-03-21 07:39:26.123744 [-] proxy profiles support is enabled, proxy profiles path: /etc/splash/proxy-profiles
   2019-03-21 07:39:26.124280 [-] memory cache: enabled, private mode: enabled, js cross-domain access: disabled
   2019-03-21 07:39:26.309697 [-] verbosity=1, slots=20, argument_cache_max_entries=500, max-timeout=90.0
   2019-03-21 07:39:26.312602 [-] Web UI: enabled, Lua: enabled (sandbox: enabled)
   2019-03-21 07:39:26.314710 [-] Site starting on 8050
   2019-03-21 07:39:26.315346 [-] Starting factory <twisted.web.server.Site object at 0x7f4a1ec41cc0>
   2019-03-21 07:39:26.316014 [-] Server listening on http://0.0.0.0:8050
   ```

4. Splash is now available at 0.0.0.0 at port 8050 (http).

   此时打开 http://localhost:8050 即可看到 Splash 的主页。

### ScrapyRedis

ScrapyRedis 是 Scrapy 分布式的扩展模块，有了它我们可以方便地实现 Scrapy 分布式爬虫的搭建，本节来介绍一下 ScrapyRedis 的安装方式。

相关资源:

- PyPI: https://pypi.org/project/scrapy-redis/
- GitHub: https://github.com/rmax/scrapy-redis
- Docs-EN: https://scrapy-redis.readthedocs.io/en/stable/

```
pip install scrapy-redis
# conda 中没有此包
```

## 与部署相关的工具

如果想要大规模抓取数据，那么一定会用到分布式爬虫，对于分布式爬虫来说，我们一定需要多台主机，每台主机多个爬虫任务，但是源代码其实只有一份。那么我们需要做的就是将一份代码同时部署到多台主机上来协同运行，那么怎么去部署就又是一个值得思考的问题。

对于 Scrapy 来说，它有一个扩展组件叫做 Scrapyd，我们只需要安装 Scrapyd 即可远程管理 Scrapy 任务，包括部署源码、启动任务、监听任务等操作。另外还有 ScrapydClient 和 ScrapydAPI 来帮助我们更方便地完成部署和监听操作。

另外还有一种部署方式就是 Docker 集群部署，我们只需要将爬虫制作为 Docker 镜像，只要主机安装了 Docker，就可以直接运行爬虫，而无需再去担心环境配置、版本问题。

### Docker

Docker 是一种容器技术，它可以将应用和环境等进行打包，形成一个独立的，类似于 iOS 的 APP 形式的「应用」，这个应用可以直接被分发到任意一个支持 Docker 的环境中，通过简单的命令即可启动运行。Docker 是一种最流行的容器化实现方案。和虚拟化技术类似，它极大的方便了应用服务的部署；又与虚拟化技术不同，它以一种更轻量的方式实现了应用服务的打包。使用 Docker 可以让每个应用彼此相互隔离，在同一台机器上同时运行多个应用，不过他们彼此之间共享同一个操作系统。Docker 的优势在于，它可以在更细的粒度上进行资源的管理，也比虚拟化技术更加节约资源。
对于爬虫来说，如果我们需要大规模部署爬虫系统的话，用 Docker 会大大提高效率，工欲善其事，必先利其器。

相关资源:

- Home: https://www.docker.com/
- GitHub: https://github.com/docker
- DockerHub：https://hub.docker.com
- Docs: https://docs.docker.com/
- Docs - Docker for Windows: https://docs.docker.com/docker-for-windows/
- Downloads-Win: https://www.docker.com/products/docker-desktop
- Docker Hub 快速入门: https://docs.docker.com/docker-hub/
- 中文社区：http://www.docker.org.cn
- DaoCloud：http://www.daocloud.io
- DaoCloud - Docs: http://guide.daocloud.io/dcs

目前收集到三个教程:

- Docker — 从入门到实践: https://github.com/yeasy/docker_practice ✨
- 测试教程网: http://www.testclass.net/docker
- runoob: http://www.runoob.com/docker/docker-tutorial.html

在 Windows 中安装 docker 的流程详见 https://docs.docker.com/docker-for-windows/install/

可使用如下命令来测试是否安装成功:

```shell
docker run hello-world
```

效果如下:

```shell
(base) C:\WINDOWS\system32>docker run hello-world
Unable to find image 'hello-world:latest' locally
latest: Pulling from library/hello-world
1b930d010525: Pull complete
Digest: sha256:2557e3c07ed1e38f26e389462d03ed943586f744621577a99efb77324b0fe535
Status: Downloaded newer image for hello-world:latest

Hello from Docker!
This message shows that your installation appears to be working correctly.

To generate this message, Docker took the following steps:
 1. The Docker client contacted the Docker daemon.
 2. The Docker daemon pulled the "hello-world" image from the Docker Hub.
    (amd64)
 3. The Docker daemon created a new container from that image which runs the
    executable that produces the output you are currently reading.
 4. The Docker daemon streamed that output to the Docker client, which sent it
    to your terminal.

To try something more ambitious, you can run an Ubuntu container with:
 $ docker run -it ubuntu bash

Share images, automate workflows, and more with a free Docker ID:
 https://hub.docker.com/

For more examples and ideas, visit:
 https://docs.docker.com/get-started/
```

#### 镜像加速器

国内从 Docker Hub 拉取镜像有时会遇到困难，此时可以配置镜像加速器。Docker 官方和国内很多云服务商都提供了国内加速器服务，例如：

- [Docker 官方提供的中国 registry mirror `https://registry.docker-cn.com`](https://docs.docker.com/registry/recipes/mirror/#use-case-the-china-registry-mirror)
  - https://www.docker-cn.com/registry-mirror 中文使用说明
- [阿里云加速器(需登录账号获取)](https://cr.console.aliyun.com/cn-hangzhou/mirrors)
- [七牛云加速器 `https://reg-mirror.qiniu.com/`](https://kirk-enterprise.github.io/hub-docs/#/user-guide/mirror)
- Docker 加速器: https://www.daocloud.io/mirror

> 当配置某一个加速器地址之后，若发现拉取不到镜像，请切换到另一个加速器地址。
>
> 国内各大云服务商均提供了 Docker 镜像加速服务，建议根据运行 Docker 的云平台选择对应的镜像加速服务。

我们以 Docker 官方加速器 `https://registry.docker-cn.com` 为例进行介绍。

对于使用 Windows 10 的系统，在系统右下角托盘 Docker 图标内右键菜单选择 `Settings`，打开配置窗口后左侧导航菜单选择 `Daemon`。在 `Registry mirrors` 一栏中填写加速器地址 `https://registry.docker-cn.com`，之后点击 `Apply` 保存后 Docker 就会重启并应用配置的镜像地址了。

### Scrapyd

Scrapyd 是用于运行 Scrapy 爬虫的服务，Scrapyd 允许你使用 HTTP JSON API 来部署 Scrapy 项目，以及控制 Scrapy 爬虫。

相关资源:

- PyPI: https://pypi.org/project/scrapyd/
- GitHub: https://github.com/scrapy/scrapyd
- Docs: https://scrapyd.readthedocs.io/en/stable/

安装:(conda 中没有 Scrapyd)

```shell
pip install scrapyd
```

Scrapyd 的配置方法可参考:

- https://scrapyd.readthedocs.io/en/stable/config.html
- Python3网络爬虫开发实战》-> 1.9 部署相关库的安装 -> Scrapyd的安装，书中仅讲解了 Linux 的配置方法。

在 Windows 下配置 Scrapyd 时，需要先配置 `c:\scrapyd\scrapyd.conf` 文件，配置文件的示例如下:(参数含义详见 [Configuration file](https://scrapyd.readthedocs.io/en/stable/config.html#config-example))

```
[scrapyd]
eggs_dir    = eggs
logs_dir    = logs
items_dir   =
jobs_to_keep = 5
dbs_dir     = dbs
max_proc    = 0
max_proc_per_cpu = 4
finished_to_keep = 100
poll_interval = 5.0
bind_address = 127.0.0.1
# bind_address = 0.0.0.0 如需通过外网访问，可设置为此值
http_port   = 6800
debug       = off
runner      = scrapyd.runner
application = scrapyd.app.application
launcher    = scrapyd.launcher.Launcher
webroot     = scrapyd.website.Root

[services]
schedule.json     = scrapyd.webservice.Schedule
cancel.json       = scrapyd.webservice.Cancel
addversion.json   = scrapyd.webservice.AddVersion
listprojects.json = scrapyd.webservice.ListProjects
listversions.json = scrapyd.webservice.ListVersions
listspiders.json  = scrapyd.webservice.ListSpiders
delproject.json   = scrapyd.webservice.DeleteProject
delversion.json   = scrapyd.webservice.DeleteVersion
listjobs.json     = scrapyd.webservice.ListJobs
daemonstatus.json = scrapyd.webservice.DaemonStatus
```

配置文件设置完毕后，直接在命令行中运行 scrapyd 便可激活服务(http://127.0.0.1:6800/)。

```shell
(spider) C:\WINDOWS\system32>scrapyd
2019-03-22T08:59:00+0800 [-] Loading c:\anaconda3\envs\spider\lib\site-packages\scrapyd\txapp.py...
2019-03-22T08:59:01+0800 [-] Scrapyd web console available at http://127.0.0.1:6800/
2019-03-22T08:59:01+0800 [-] Loaded.
2019-03-22T08:59:01+0800 [twisted.application.app.AppLogger#info] twistd 18.9.0 (c:\anaconda3\envs\spider\python.exe 3.7.2) starting up.
2019-03-22T08:59:01+0800 [twisted.application.app.AppLogger#info] reactor class: twisted.internet.selectreactor.SelectReactor.
2019-03-22T08:59:01+0800 [-] Site starting on 6800
2019-03-22T08:59:01+0800 [twisted.web.server.Site#info] Starting factory <twisted.web.server.Site object at 0x00000256934AED68>
2019-03-22T08:59:01+0800 [Launcher] Scrapyd 1.2.0 started: max_proc=16, runner='scrapyd.runner'
```

运行 Scrapyd 更佳的方式是使用 Supervisor 守护进程运行，可以参考：<http://supervisord.org/>。

Scrapyd 也支持 Docker。

### Scrapyd-Client

Scrapyd-client is a client for [Scrapyd](https://scrapyd.readthedocs.io/). It provides the general `scrapyd-client` and the `scrapyd-deploy` utility which allows you to deploy your project to a Scrapyd server.

在将 Scrapy 代码部署到远程 Scrapyd 的时候，其第一步就是要将代码打包为 Egg 文件，其次需要将 Egg 文件上传到远程主机，这个过程如果我们用程序来实现是完全可以的，但是我们并不需要做这些工作，因为 ScrapydClient 已经为我们实现了这些功能。

相关资源:

- GitHub: https://github.com/scrapy/scrapyd-client
- PyPi: https://pypi.python.org/pypi/scrapyd-client
- Docs: https://github.com/scrapy/scrapyd-client#scrapyd-deploy

安装:(conda 中没有此包)

```
pip3 install scrapyd-client
```

在安装完成后，尝试通过如下操作来验证安装是否成功，但是不能获得期望的输出，待需要用到 Scrapyd-client 时，再行研究。

> 安装完成后，可使用部署命令 `scrapyd-deploy` 来验证安装是否成功:
>
> ```python
> scrapyd-deploy -h
> ```
>

### ScrapydAPI

Allows a Python application to talk to, and therefore control, the [Scrapy](http://scrapy.org/) daemon: [Scrapyd](https://github.com/scrapy/scrapyd).

相关资源:

- GitHub：<https://pypi.python.org/pypi/python-scrapyd-api/>
- PyPi：<https://pypi.python.org/pypi/python-scrapyd-api>
- 官方文档：<http://python-scrapyd-api.readthedocs.io/en/latest/usage.html>

安装好了 Scrapyd 之后，我们可以直接请求它提供的 API 即可获取当前主机的 Scrapy 任务运行状况。

如某台主机的 IP 为 192.168.1.1，则可以直接运行如下命令获取当前主机的所有 Scrapy 项目：

```
curl http://localhost:6800/listprojects.json
```

运行结果：

```
{"status": "ok", "projects": ["myproject", "otherproject"]}
```

返回结果是 Json 字符串，通过解析这个字符串我们便可以得到当前主机所有项目。

但是用这种方式来获取任务状态还是有点繁琐，所以 ScrapydAPI 就为它做了一层封装，下面我们来看下它的安装方式。

安装

```
pip install python-scrapyd-api
# conda中没有此包
```

安装完成之后便可以使用 Python 来获取主机状态了，所以如上的操作便可以用 Python 代码实现：

```python
from scrapyd_api import ScrapydAPI
scrapyd = ScrapydAPI('http://localhost:6800')
print(scrapyd.list_projects())
```

运行结果：

```python
["myproject", "otherproject"]
```

这样我们便可以用 Python 直接来获取各个主机上 Scrapy 任务的运行状态了。

### Scrapyrt的安装

Scrapyrt 为 Scrapy 提供了一个调度的 HTTP 接口，有了它我们不需要再执行 Scrapy 命令而是通过请求一个 HTTP 接口即可调度 Scrapy 任务，Scrapyrt 比 Scrapyd 轻量级，如果不需要分布式多任务的话可以简单使用 Scrapyrt 实现远程 Scrapy 任务的调度。

相关资源

- GitHub：https://github.com/scrapinghub/scrapyrt
- Docs：https://scrapyrt.readthedocs.io/en/latest/index.html

推荐使用 Pip 安装，命令如下：

```
pip3 install scrapyrt
```

命令执行完毕之后即可完成安装。

接下来在任意一个 Scrapy 项目中运行如下命令即可启动 HTTP 服务：

```
scrapyrt
```

运行之后会默认在 9080 端口上启动服务，类似的输出结果如下：

```
scrapyrt
2017-07-12 22:31:03+0800 [-] Log opened.
2017-07-12 22:31:03+0800 [-] Site starting on 9080
2017-07-12 22:31:03+0800 [-] Starting factory <twisted.web.server.Site object at 0x10294b160>
```

如果想更换运行端口可以使用 -p 参数，如：

```
scrapyrt -p 9081
```

这样就会在 9081 端口上运行了。

另外 Scrapyrt 也支持 Docker，如想要在 9080 端口上运行，且本地 Scrapy 项目的路径为 /home/quotesbot，可以使用如下命令运行：

```
docker run -p 9080:9080 -tid -v /home/user/quotesbot:/scrapyrt/project scrapinghub/scrapyrt
```

这样同样可以在 9080 端口上监听指定的 Scrapy 项目。

### Gerapy

Gerapy 是一个 Scrapy 分布式管理模块，本节来介绍一下 Gerapy 的安装方式。

相关资源:

- GitHub：<https://github.com/Gerapy>

推荐使用 Pip 安装，命令如下：

```
pip3 install gerapy
```

安装完成之后，可以在 Python 命令行下测试。

```python
$ python3
>>> import gerapy
```

如果没有错误报出，则证明库已经安装好了。







































